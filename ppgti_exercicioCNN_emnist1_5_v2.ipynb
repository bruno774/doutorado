{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "A100",
      "mount_file_id": "1f03cEOMqkxGsjCeFFEyX2hJdL_uTyQE8",
      "authorship_tag": "ABX9TyNQw1ZYRl8rF4yhIedwBZeH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bruno774/doutorado/blob/main/ppgti_exercicioCNN_emnist1_5_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fed26ba7"
      },
      "source": [
        "# Atividade da Disciplina PPGTI3003 - Aprendizado Profundo - T01 (2025.2)\n",
        "## Prof. Josenalde Barbosa de Oliveira\n",
        "\n",
        "Implementar uma arquitetura de Rede Neural Convolucional (CNN) baseada nos experimentos descritos no artigo *Classifica√ß√£o de Caracteres Manuscritos para Corre√ß√£o Autom√°tica do Sistema Multiprova (Silva Filho e outros)*.\n",
        "\n",
        "Foi identificada no artigo qual a melhor arquitetura para o problema de classifica√ß√£o dos d√≠gitos 1 a 5, V e F, A a E como sendo uma CNN identificada como Estrutura 4. O modelo deve ser treinado e testado no conjunto de dados EMNIST, com foco espec√≠fico nos d√≠gitos de 1 a 5.\n",
        "\n",
        "A tarefa inclui baixar e preparar o conjunto de dados EMNIST, implementar a CNN em PyTorch, treinar o modelo, avaliar seu desempenho com m√©tricas como acur√°cia e visualizar os resultados (curvas de perda/acur√°cia e, potencialmente, previs√µes)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1f373fb"
      },
      "source": [
        "## Definir Arquitetura CNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f37ee76"
      },
      "source": [
        "### Detalhes arquiteturais identificados no artigo\n",
        "\n",
        "As especifica√ß√µes arquiteturais para a 'Estrutura 4' do artigo `silvafilho2022.pdf` definem a seguinte estrutura:\n",
        "1.  **Camadas Convolucionais (`Conv2d`):** 4 blocos (32 -> 64 -> 64 -> 64 filtros (canais de sa√≠da)), tamanho do kernel 3x3 com padding.\n",
        "2.  **Camadas de Pooling (`MaxPool2d`):\n",
        "):** MaxPooling 2x2.\n",
        "3.  **Fun√ß√£o de Ativa√ß√£o:** ReLU.\n",
        "4.  **Camadas Totalmente Conectadas (Fully Connected Layers):** 256 -> 5 classes.\n",
        "5.  **Camada de Sa√≠da:** N√∫mero de classes (que √© 5, para d√≠gitos 1-5) e fun√ß√£o de ativa√ß√£o final (se aplic√°vel, e.g., Softmax).\n",
        "\n",
        "Camada Flatten\n",
        "\n",
        "Configura√ß√µes de treinamento alcan√ßado no experimento:\n",
        "\n",
        "    * Optimizer: Adam\n",
        "    * Imagens de entrada: 32x32x3 pixels\n",
        "    * Early stopping com margem de 0,1%\n",
        "    * Divis√£o do dataset: 70% treino, 15% valida√ß√£o, 15% teste"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### carregando bibliotecas do projeto"
      ],
      "metadata": {
        "id": "fFxn6QjatkOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import EMNIST\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "metadata": {
        "id": "e34r0D3_s4-k"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### estruturas e defs"
      ],
      "metadata": {
        "id": "x-j7efh1tqH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNEstrutura4(nn.Module):\n",
        "    \"\"\"\n",
        "    Arquitetura CNN Estrutura 4 conforme descrito no artigo.\n",
        "\n",
        "    Arquitetura:\n",
        "    - 4 blocos convolucionais (Conv2d + ReLU + MaxPool2d)\n",
        "    - Camada Flatten\n",
        "    - Camada totalmente conectada\n",
        "\n",
        "    Especifica√ß√µes:\n",
        "    - 1¬™ camada: 32 filtros 3x3\n",
        "    - 2¬™-4¬™ camadas: 64 filtros 3x3\n",
        "    - Max pooling: 2x2\n",
        "    - Fun√ß√£o de ativa√ß√£o: ReLU\n",
        "    - Input: 32x32x3 (conforme artigo, mas adaptado para grayscale)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=5):\n",
        "        super(CNNEstrutura4, self).__init__()\n",
        "\n",
        "        # Bloco Convolucional 1: 32 filtros 3x3\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Bloco Convolucional 2: 64 filtros 3x3\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Bloco Convolucional 3: 64 filtros 3x3\n",
        "        self.conv_block3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Bloco Convolucional 4: 64 filtros 3x3\n",
        "        self.conv_block4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Camada totalmente conectada\n",
        "        # Ap√≥s 4 max poolings de 2x2, 32x32 -> 2x2\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(64 * 2 * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block1(x)\n",
        "        x = self.conv_block2(x)\n",
        "        x = self.conv_block3(x)\n",
        "        x = self.conv_block4(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EMNISTDigitsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset customizado para filtrar apenas d√≠gitos 1-5 do EMNIST\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, emnist_dataset, target_digits=[1, 2, 3, 4, 5]):\n",
        "        self.data = []\n",
        "        self.targets = []\n",
        "\n",
        "        # Filtrar apenas os d√≠gitos desejados\n",
        "        for idx in range(len(emnist_dataset)):\n",
        "            img, label = emnist_dataset[idx]\n",
        "            if label in target_digits:\n",
        "                self.data.append(img)\n",
        "                # Remapear labels para 0-4\n",
        "                self.targets.append(target_digits.index(label))\n",
        "\n",
        "        print(f\"Dataset filtrado: {len(self.data)} amostras dos d√≠gitos {target_digits}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.targets[idx]\n",
        "\n",
        "\n",
        "def prepare_data(data_dir='./data', batch_size=64):\n",
        "    \"\"\"\n",
        "    Prepara os dados EMNIST conforme especifica√ß√µes do artigo:\n",
        "    - 70% treino\n",
        "    - 15% valida√ß√£o\n",
        "    - 15% teste\n",
        "    - Imagens 32x32\n",
        "    \"\"\"\n",
        "\n",
        "    # Transforma√ß√µes: redimensionar para 32x32 e normalizar\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    # Carregar EMNIST digits\n",
        "    print(\"Carregando dataset EMNIST...\")\n",
        "    full_dataset = EMNIST(root=data_dir, split='digits', train=True,\n",
        "                          download=True, transform=transform)\n",
        "\n",
        "    # Filtrar apenas d√≠gitos 1-5\n",
        "    filtered_dataset = EMNISTDigitsDataset(full_dataset, target_digits=[1, 2, 3, 4, 5])\n",
        "\n",
        "    # Dividir dataset: 70% treino, 15% valida√ß√£o, 15% teste\n",
        "    total_size = len(filtered_dataset)\n",
        "    train_size = int(0.70 * total_size)\n",
        "    val_size = int(0.15 * total_size)\n",
        "    test_size = total_size - train_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = random_split(\n",
        "        filtered_dataset, [train_size, val_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    print(f\"\\nDivis√£o do dataset:\")\n",
        "    print(f\"Treino: {train_size} ({train_size/total_size*100:.1f}%)\")\n",
        "    print(f\"Valida√ß√£o: {val_size} ({val_size/total_size*100:.1f}%)\")\n",
        "    print(f\"Teste: {test_size} ({test_size/total_size*100:.1f}%)\")\n",
        "\n",
        "    # Criar DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"\n",
        "    Early stopping conforme descrito no artigo:\n",
        "    Para quando n√£o h√° melhora de 0.1% na precis√£o ou perda\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, patience=5, min_delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.best_model = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_model = model.state_dict().copy()\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_model = model.state_dict().copy()\n",
        "            self.counter = 0\n",
        "\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Treina o modelo por uma √©poca\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc='Treinamento')\n",
        "    for inputs, labels in pbar:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'loss': running_loss / (pbar.n + 1),\n",
        "            'acc': 100. * correct / total\n",
        "        })\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"Valida o modelo\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(val_loader, desc='Valida√ß√£o'):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=0.001, device='cuda'):\n",
        "    \"\"\"\n",
        "    Treina o modelo usando Adam optimizer conforme artigo\n",
        "    Implementa early stopping\n",
        "    \"\"\"\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
        "\n",
        "    # Listas para armazenar hist√≥rico\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "\n",
        "    print(f\"\\nIniciando treinamento no dispositivo: {device}\")\n",
        "    print(f\"Optimizer: Adam (lr={learning_rate})\")\n",
        "    print(f\"Crit√©rio: CrossEntropyLoss\")\n",
        "    print(f\"Early Stopping: margem de 0.1%\\n\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\n√âpoca {epoch+1}/{num_epochs}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Treinar\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "        # Validar\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        # Armazenar hist√≥rico\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"\\nResumo da √âpoca {epoch+1}:\")\n",
        "        print(f\"  Treino    - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
        "        print(f\"  Valida√ß√£o - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Early stopping\n",
        "        early_stopping(val_loss, model)\n",
        "        if early_stopping.early_stop:\n",
        "            print(f\"\\nEarly stopping acionado na √©poca {epoch+1}\")\n",
        "            model.load_state_dict(early_stopping.best_model)\n",
        "            break\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"\n",
        "    Avalia o modelo no conjunto de teste\n",
        "    Retorna m√©tricas detalhadas e matriz de confus√£o\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader, desc='Avalia√ß√£o'):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "\n",
        "    # Matriz de confus√£o\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    # Relat√≥rio de classifica√ß√£o\n",
        "    target_names = ['D√≠gito 1', 'D√≠gito 2', 'D√≠gito 3', 'D√≠gito 4', 'D√≠gito 5']\n",
        "    report = classification_report(all_labels, all_predictions,\n",
        "                                   target_names=target_names,\n",
        "                                   digits=4)\n",
        "\n",
        "    return accuracy, cm, report\n",
        "\n",
        "\n",
        "def plot_training_history(history, save_path='training_history.png'):\n",
        "    \"\"\"\n",
        "    Plota gr√°ficos de perda e precis√£o conforme artigo\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    # Gr√°fico de Perda\n",
        "    ax1.plot(epochs, history['train_loss'], 'b-', label='Treino', linewidth=2)\n",
        "    ax1.plot(epochs, history['val_loss'], 'r--', label='Valida√ß√£o', linewidth=2)\n",
        "    ax1.set_xlabel('√âpocas', fontsize=12)\n",
        "    ax1.set_ylabel('Perda', fontsize=12)\n",
        "    ax1.set_title('Perda do modelo', fontsize=14, fontweight='bold')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Gr√°fico de Precis√£o\n",
        "    ax2.plot(epochs, history['train_acc'], 'b-', label='Treino', linewidth=2)\n",
        "    ax2.plot(epochs, history['val_acc'], 'r--', label='Valida√ß√£o', linewidth=2)\n",
        "    ax2.set_xlabel('√âpocas', fontsize=12)\n",
        "    ax2.set_ylabel('Precis√£o (%)', fontsize=12)\n",
        "    ax2.set_title('Precis√£o do modelo', fontsize=14, fontweight='bold')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"\\nGr√°ficos de treinamento salvos em: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, save_path='confusion_matrix.png'):\n",
        "    \"\"\"\n",
        "    Plota matriz de confus√£o conforme artigo\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Normalizar para percentuais\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "    # Criar heatmap\n",
        "    sns.heatmap(cm_normalized, annot=True, fmt='.1f', cmap='Blues',\n",
        "                xticklabels=['1', '2', '3', '4', '5'],\n",
        "                yticklabels=['1', '2', '3', '4', '5'],\n",
        "                cbar_kws={'label': 'Percentual (%)'},\n",
        "                vmin=0, vmax=100)\n",
        "\n",
        "    plt.xlabel('Predi√ß√£o', fontsize=12)\n",
        "    plt.ylabel('Real', fontsize=12)\n",
        "    plt.title('Matriz de Confus√£o - D√≠gitos 1-5', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"Matriz de confus√£o salva em: {save_path}\")\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "6orBQ33YtfUe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### execu√ß√£o de carga dos dados e treinamento"
      ],
      "metadata": {
        "id": "jjD9HoGTt0jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Fun√ß√£o principal para executar o pipeline completo\n",
        "    \"\"\"\n",
        "\n",
        "    # Configura√ß√µes\n",
        "    BATCH_SIZE = 64\n",
        "    NUM_EPOCHS = 50\n",
        "    LEARNING_RATE = 0.001\n",
        "    DATA_DIR = './data'\n",
        "\n",
        "    # Verificar GPU\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Dispositivo utilizado: {device}\")\n",
        "\n",
        "    # Preparar dados\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PREPARA√á√ÉO DOS DADOS\")\n",
        "    print(\"=\"*70)\n",
        "    train_loader, val_loader, test_loader = prepare_data(DATA_DIR, BATCH_SIZE)\n",
        "\n",
        "    # Criar modelo\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"CRIA√á√ÉO DO MODELO - ESTRUTURA 4\")\n",
        "    print(\"=\"*70)\n",
        "    model = CNNEstrutura4(num_classes=5).to(device)\n",
        "\n",
        "    # Contar par√¢metros\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"\\nPar√¢metros totais: {total_params:,}\")\n",
        "    print(f\"Par√¢metros trein√°veis: {trainable_params:,}\")\n",
        "\n",
        "    print(\"\\nArquitetura do modelo:\")\n",
        "    print(model)\n",
        "\n",
        "    # Treinar modelo\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"TREINAMENTO\")\n",
        "    print(\"=\"*70)\n",
        "    history = train_model(model, train_loader, val_loader,\n",
        "                         num_epochs=NUM_EPOCHS,\n",
        "                         learning_rate=LEARNING_RATE,\n",
        "                         device=device)\n",
        "\n",
        "    # Plotar hist√≥rico de treinamento\n",
        "    plot_training_history(history)\n",
        "\n",
        "    # Avaliar modelo\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"AVALIA√á√ÉO NO CONJUNTO DE TESTE\")\n",
        "    print(\"=\"*70)\n",
        "    accuracy, cm, report = evaluate_model(model, test_loader, device)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"RESULTADOS FINAIS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"\\nPrecis√£o no teste: {accuracy:.2f}%\")\n",
        "    print(f\"\\nRelat√≥rio de Classifica√ß√£o:\")\n",
        "    print(report)\n",
        "\n",
        "    # Plotar matriz de confus√£o\n",
        "    plot_confusion_matrix(cm)\n",
        "\n",
        "    # Salvar modelo\n",
        "    model_path = 'cnn_estrutura4_digits_1-5.pth'\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'accuracy': accuracy,\n",
        "        'confusion_matrix': cm,\n",
        "        'history': history\n",
        "    }, model_path)\n",
        "    print(f\"\\nModelo salvo em: {model_path}\")\n",
        "\n",
        "    # Compara√ß√£o com resultados do artigo\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"COMPARA√á√ÉO COM O ARTIGO\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Resultado do artigo (Estrutura 4, d√≠gitos 0-9): 98,84%\")\n",
        "    print(f\"Resultado obtido (d√≠gitos 1-5): {accuracy:.2f}%\")\n",
        "\n",
        "    # Calcular perda final\n",
        "    final_train_loss = history['train_loss'][-1]\n",
        "    final_val_loss = history['val_loss'][-1]\n",
        "    print(f\"\\nPerda final (treino): {final_train_loss:.4f}\")\n",
        "    print(f\"Perda final (valida√ß√£o): {final_val_loss:.4f}\")\n",
        "    print(f\"Perda no artigo (Estrutura 4): 0,064\")\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"PIPELINE CONCLU√çDO COM SUCESSO!\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcnDq5nqtzdX",
        "outputId": "856c0143-d874-497e-c93d-ff75bc185ac9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dispositivo utilizado: cuda\n",
            "\n",
            "======================================================================\n",
            "PREPARA√á√ÉO DOS DADOS\n",
            "======================================================================\n",
            "Carregando dataset EMNIST...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 562M/562M [00:02<00:00, 205MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset filtrado: 120000 amostras dos d√≠gitos [1, 2, 3, 4, 5]\n",
            "\n",
            "Divis√£o do dataset:\n",
            "Treino: 84000 (70.0%)\n",
            "Valida√ß√£o: 18000 (15.0%)\n",
            "Teste: 18000 (15.0%)\n",
            "\n",
            "======================================================================\n",
            "CRIA√á√ÉO DO MODELO - ESTRUTURA 4\n",
            "======================================================================\n",
            "\n",
            "Par√¢metros totais: 93,957\n",
            "Par√¢metros trein√°veis: 93,957\n",
            "\n",
            "Arquitetura do modelo:\n",
            "CNNEstrutura4(\n",
            "  (conv_block1): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv_block2): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv_block3): Sequential(\n",
            "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv_block4): Sequential(\n",
            "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "\n",
            "======================================================================\n",
            "TREINAMENTO\n",
            "======================================================================\n",
            "\n",
            "Iniciando treinamento no dispositivo: cuda\n",
            "Optimizer: Adam (lr=0.001)\n",
            "Crit√©rio: CrossEntropyLoss\n",
            "Early Stopping: margem de 0.1%\n",
            "\n",
            "\n",
            "√âpoca 1/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinamento: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1313/1313 [00:08<00:00, 158.61it/s, loss=0.054, acc=98.3]\n",
            "Valida√ß√£o: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [00:00<00:00, 422.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resumo da √âpoca 1:\n",
            "  Treino    - Loss: 0.0540, Acc: 98.25%\n",
            "  Valida√ß√£o - Loss: 0.0159, Acc: 99.53%\n",
            "\n",
            "√âpoca 2/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinamento: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1313/1313 [00:06<00:00, 210.86it/s, loss=0.0146, acc=99.5]\n",
            "Valida√ß√£o: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [00:00<00:00, 424.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resumo da √âpoca 2:\n",
            "  Treino    - Loss: 0.0144, Acc: 99.55%\n",
            "  Valida√ß√£o - Loss: 0.0111, Acc: 99.65%\n",
            "\n",
            "√âpoca 3/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinamento: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1313/1313 [00:06<00:00, 204.26it/s, loss=0.0111, acc=99.7]\n",
            "Valida√ß√£o: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [00:00<00:00, 418.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resumo da √âpoca 3:\n",
            "  Treino    - Loss: 0.0110, Acc: 99.69%\n",
            "  Valida√ß√£o - Loss: 0.0114, Acc: 99.65%\n",
            "\n",
            "√âpoca 4/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinamento: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1313/1313 [00:06<00:00, 209.87it/s, loss=0.00927, acc=99.7]\n",
            "Valida√ß√£o: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [00:00<00:00, 398.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resumo da √âpoca 4:\n",
            "  Treino    - Loss: 0.0092, Acc: 99.72%\n",
            "  Valida√ß√£o - Loss: 0.0094, Acc: 99.72%\n",
            "\n",
            "√âpoca 5/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinamento: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1313/1313 [00:06<00:00, 205.91it/s, loss=0.00791, acc=99.8]\n",
            "Valida√ß√£o: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [00:00<00:00, 381.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resumo da √âpoca 5:\n",
            "  Treino    - Loss: 0.0079, Acc: 99.76%\n",
            "  Valida√ß√£o - Loss: 0.0087, Acc: 99.76%\n",
            "\n",
            "√âpoca 6/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinamento: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1313/1313 [00:06<00:00, 208.85it/s, loss=0.00638, acc=99.8]\n",
            "Valida√ß√£o: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [00:00<00:00, 433.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resumo da √âpoca 6:\n",
            "  Treino    - Loss: 0.0064, Acc: 99.81%\n",
            "  Valida√ß√£o - Loss: 0.0093, Acc: 99.73%\n",
            "\n",
            "√âpoca 7/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinamento: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1313/1313 [00:06<00:00, 210.36it/s, loss=0.00577, acc=99.8]\n",
            "Valida√ß√£o: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [00:00<00:00, 428.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resumo da √âpoca 7:\n",
            "  Treino    - Loss: 0.0057, Acc: 99.84%\n",
            "  Valida√ß√£o - Loss: 0.0120, Acc: 99.67%\n",
            "\n",
            "√âpoca 8/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinamento: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1313/1313 [00:06<00:00, 209.67it/s, loss=0.00508, acc=99.8]\n",
            "Valida√ß√£o: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [00:00<00:00, 417.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resumo da √âpoca 8:\n",
            "  Treino    - Loss: 0.0050, Acc: 99.84%\n",
            "  Valida√ß√£o - Loss: 0.0115, Acc: 99.67%\n",
            "\n",
            "√âpoca 9/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinamento: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1313/1313 [00:06<00:00, 211.05it/s, loss=0.00414, acc=99.9]\n",
            "Valida√ß√£o: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [00:00<00:00, 406.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resumo da √âpoca 9:\n",
            "  Treino    - Loss: 0.0041, Acc: 99.88%\n",
            "  Valida√ß√£o - Loss: 0.0080, Acc: 99.77%\n",
            "\n",
            "√âpoca 10/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinamento: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1313/1313 [00:06<00:00, 208.93it/s, loss=0.00331, acc=99.9]\n",
            "Valida√ß√£o: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [00:00<00:00, 365.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resumo da √âpoca 10:\n",
            "  Treino    - Loss: 0.0033, Acc: 99.91%\n",
            "  Valida√ß√£o - Loss: 0.0111, Acc: 99.72%\n",
            "\n",
            "√âpoca 11/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinamento: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1313/1313 [00:06<00:00, 202.30it/s, loss=0.00419, acc=99.9]\n",
            "Valida√ß√£o: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [00:00<00:00, 396.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resumo da √âpoca 11:\n",
            "  Treino    - Loss: 0.0042, Acc: 99.87%\n",
            "  Valida√ß√£o - Loss: 0.0085, Acc: 99.79%\n",
            "\n",
            "√âpoca 12/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinamento: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1313/1313 [00:06<00:00, 198.14it/s, loss=0.00319, acc=99.9]\n",
            "Valida√ß√£o: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [00:00<00:00, 393.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resumo da √âpoca 12:\n",
            "  Treino    - Loss: 0.0032, Acc: 99.91%\n",
            "  Valida√ß√£o - Loss: 0.0118, Acc: 99.62%\n",
            "\n",
            "√âpoca 13/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinamento: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1313/1313 [00:06<00:00, 193.92it/s, loss=0.00226, acc=99.9]\n",
            "Valida√ß√£o: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [00:00<00:00, 396.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resumo da √âpoca 13:\n",
            "  Treino    - Loss: 0.0022, Acc: 99.92%\n",
            "  Valida√ß√£o - Loss: 0.0122, Acc: 99.73%\n",
            "\n",
            "√âpoca 14/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Treinamento: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1313/1313 [00:06<00:00, 195.08it/s, loss=0.00295, acc=99.9]\n",
            "Valida√ß√£o: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [00:00<00:00, 423.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resumo da √âpoca 14:\n",
            "  Treino    - Loss: 0.0029, Acc: 99.91%\n",
            "  Valida√ß√£o - Loss: 0.0112, Acc: 99.72%\n",
            "\n",
            "Early stopping acionado na √©poca 14\n",
            "\n",
            "Gr√°ficos de treinamento salvos em: training_history.png\n",
            "\n",
            "======================================================================\n",
            "AVALIA√á√ÉO NO CONJUNTO DE TESTE\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Avalia√ß√£o: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [00:00<00:00, 418.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "RESULTADOS FINAIS\n",
            "======================================================================\n",
            "\n",
            "Precis√£o no teste: 99.76%\n",
            "\n",
            "Relat√≥rio de Classifica√ß√£o:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    D√≠gito 1     0.9978    0.9992    0.9985      3578\n",
            "    D√≠gito 2     0.9986    0.9972    0.9979      3612\n",
            "    D√≠gito 3     0.9966    0.9943    0.9954      3503\n",
            "    D√≠gito 4     0.9992    0.9984    0.9988      3646\n",
            "    D√≠gito 5     0.9956    0.9986    0.9971      3661\n",
            "\n",
            "    accuracy                         0.9976     18000\n",
            "   macro avg     0.9976    0.9975    0.9975     18000\n",
            "weighted avg     0.9976    0.9976    0.9976     18000\n",
            "\n",
            "Matriz de confus√£o salva em: confusion_matrix.png\n",
            "\n",
            "Modelo salvo em: cnn_estrutura4_digits_1-5.pth\n",
            "\n",
            "======================================================================\n",
            "COMPARA√á√ÉO COM O ARTIGO\n",
            "======================================================================\n",
            "Resultado do artigo (Estrutura 4, d√≠gitos 0-9): 98,84%\n",
            "Resultado obtido (d√≠gitos 1-5): 99.76%\n",
            "\n",
            "Perda final (treino): 0.0029\n",
            "Perda final (valida√ß√£o): 0.0112\n",
            "Perda no artigo (Estrutura 4): 0,064\n",
            "\n",
            "======================================================================\n",
            "PIPELINE CONCLU√çDO COM SUCESSO!\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tqKACduWxfcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### an√°lise dos resultados"
      ],
      "metadata": {
        "id": "sD7awiYjuQGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_results(model_path='cnn_estrutura4_digits_1-5.pth'):\n",
        "    \"\"\"Carrega os resultados salvos do modelo\"\"\"\n",
        "    checkpoint = torch.load(model_path, map_location='cuda', weights_only=False)\n",
        "    return checkpoint\n",
        "\n",
        "\n",
        "def analyze_confusion_matrix(cm, class_names=['1', '2', '3', '4', '5']):\n",
        "    \"\"\"\n",
        "    An√°lise detalhada da matriz de confus√£o\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"AN√ÅLISE DA MATRIZ DE CONFUS√ÉO\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Normalizar para percentuais\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "    # An√°lise diagonal (acertos)\n",
        "    print(\"\\nüìä Taxa de Acerto por Classe:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        accuracy = cm_normalized[i, i]\n",
        "        print(f\"  D√≠gito {class_name}: {accuracy:.2f}%\")\n",
        "\n",
        "    # Melhor e pior classe\n",
        "    diagonal = np.diag(cm_normalized)\n",
        "    best_class = np.argmax(diagonal)\n",
        "    worst_class = np.argmin(diagonal)\n",
        "\n",
        "    print(f\"\\n‚úÖ Melhor classe: D√≠gito {class_names[best_class]} ({diagonal[best_class]:.2f}%)\")\n",
        "    print(f\"‚ö†Ô∏è  Pior classe: D√≠gito {class_names[worst_class]} ({diagonal[worst_class]:.2f}%)\")\n",
        "\n",
        "    # An√°lise de confus√µes (fora da diagonal)\n",
        "    print(\"\\nüîÄ Principais Confus√µes:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    confusions = []\n",
        "    for i in range(len(class_names)):\n",
        "        for j in range(len(class_names)):\n",
        "            if i != j and cm_normalized[i, j] > 0:\n",
        "                confusions.append((i, j, cm_normalized[i, j]))\n",
        "\n",
        "    # Ordenar por taxa de confus√£o\n",
        "    confusions.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    # Mostrar top 5 confus√µes\n",
        "    for idx, (i, j, conf_rate) in enumerate(confusions[:5], 1):\n",
        "        print(f\"  {idx}. D√≠gito {class_names[i]} confundido com {class_names[j]}: {conf_rate:.2f}%\")\n",
        "\n",
        "    if len(confusions) == 0:\n",
        "        print(\"  Nenhuma confus√£o significativa detectada!\")\n",
        "\n",
        "    # Erro m√°ximo\n",
        "    max_confusion = confusions[0][2] if confusions else 0\n",
        "    print(f\"\\n‚ùå Taxa m√°xima de erro: {max_confusion:.2f}%\")\n",
        "\n",
        "    return cm_normalized, diagonal, confusions\n",
        "\n",
        "\n",
        "def compare_with_article(accuracy, loss, cm):\n",
        "    \"\"\"\n",
        "    Compara os resultados obtidos com os do artigo\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"COMPARA√á√ÉO COM OS RESULTADOS DO ARTIGO\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Dados do artigo - Estrutura 4 (d√≠gitos 0-9)\n",
        "    article_results = {\n",
        "        'accuracy': 98.84,\n",
        "        'loss': 0.064,\n",
        "        'min_class_accuracy': 98.0,\n",
        "        'max_error': 0.9,\n",
        "        'epochs': 24,\n",
        "        'training_time': 150  # segundos\n",
        "    }\n",
        "\n",
        "    print(\"\\nüìÑ Resultados do Artigo (Estrutura 4, d√≠gitos 0-9):\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"  Precis√£o geral: {article_results['accuracy']:.2f}%\")\n",
        "    print(f\"  Perda: {article_results['loss']:.3f}\")\n",
        "    print(f\"  Taxa m√≠nima por classe: {article_results['min_class_accuracy']:.2f}%\")\n",
        "    print(f\"  Erro m√°ximo entre classes: {article_results['max_error']:.2f}%\")\n",
        "    print(f\"  √âpocas de treinamento: {article_results['epochs']}\")\n",
        "    print(f\"  Tempo de treinamento: ~{article_results['training_time']} segundos\")\n",
        "\n",
        "    # Calcular m√©tricas do modelo atual\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "    diagonal = np.diag(cm_normalized)\n",
        "    min_class_acc = diagonal.min()\n",
        "\n",
        "    # Confus√µes\n",
        "    max_error = 0\n",
        "    for i in range(len(cm_normalized)):\n",
        "        for j in range(len(cm_normalized)):\n",
        "            if i != j:\n",
        "                max_error = max(max_error, cm_normalized[i, j])\n",
        "\n",
        "    print(\"\\nüî¨ Resultados Obtidos (d√≠gitos 1-5):\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"  Precis√£o geral: {accuracy:.2f}%\")\n",
        "    print(f\"  Perda: {loss:.3f}\")\n",
        "    print(f\"  Taxa m√≠nima por classe: {min_class_acc:.2f}%\")\n",
        "    print(f\"  Erro m√°ximo entre classes: {max_error:.2f}%\")\n",
        "\n",
        "    # An√°lise comparativa\n",
        "    print(\"\\nüìä An√°lise Comparativa:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    diff_accuracy = accuracy - article_results['accuracy']\n",
        "    diff_loss = loss - article_results['loss']\n",
        "    diff_min_acc = min_class_acc - article_results['min_class_accuracy']\n",
        "    diff_max_error = max_error - article_results['max_error']\n",
        "\n",
        "    print(f\"  Diferen√ßa na precis√£o: {diff_accuracy:+.2f}% \", end=\"\")\n",
        "    print(\"‚úÖ\" if diff_accuracy >= -1 else \"‚ö†Ô∏è\")\n",
        "\n",
        "    print(f\"  Diferen√ßa na perda: {diff_loss:+.3f} \", end=\"\")\n",
        "    print(\"‚úÖ\" if diff_loss <= 0.01 else \"‚ö†Ô∏è\")\n",
        "\n",
        "    print(f\"  Diferen√ßa taxa m√≠n. classe: {diff_min_acc:+.2f}% \", end=\"\")\n",
        "    print(\"‚úÖ\" if diff_min_acc >= -2 else \"‚ö†Ô∏è\")\n",
        "\n",
        "    print(f\"  Diferen√ßa erro m√°ximo: {diff_max_error:+.2f}% \", end=\"\")\n",
        "    print(\"‚úÖ\" if diff_max_error <= 1 else \"‚ö†Ô∏è\")\n",
        "\n",
        "    # Conclus√£o\n",
        "    print(\"\\nüí° Conclus√£o:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    if accuracy >= 98.0 and max_error <= 2.0:\n",
        "        print(\"  ‚úÖ Resultados EXCELENTES! Compar√°veis ao artigo.\")\n",
        "    elif accuracy >= 97.0 and max_error <= 3.0:\n",
        "        print(\"  ‚úÖ Resultados MUITO BONS! Pr√≥ximos ao artigo.\")\n",
        "    elif accuracy >= 95.0:\n",
        "        print(\"  ‚ö†Ô∏è  Resultados BONS, mas podem ser melhorados.\")\n",
        "    else:\n",
        "        print(\"  ‚ùå Resultados abaixo do esperado. Revisar configura√ß√µes.\")\n",
        "\n",
        "    return {\n",
        "        'article': article_results,\n",
        "        'obtained': {\n",
        "            'accuracy': accuracy,\n",
        "            'loss': loss,\n",
        "            'min_class_accuracy': min_class_acc,\n",
        "            'max_error': max_error\n",
        "        },\n",
        "        'differences': {\n",
        "            'accuracy': diff_accuracy,\n",
        "            'loss': diff_loss,\n",
        "            'min_class_accuracy': diff_min_acc,\n",
        "            'max_error': diff_max_error\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def analyze_training_history(history):\n",
        "    \"\"\"\n",
        "    An√°lise detalhada do hist√≥rico de treinamento\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"AN√ÅLISE DO HIST√ìRICO DE TREINAMENTO\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    train_loss = history['train_loss']\n",
        "    val_loss = history['val_loss']\n",
        "    train_acc = history['train_acc']\n",
        "    val_acc = history['val_acc']\n",
        "\n",
        "    num_epochs = len(train_loss)\n",
        "\n",
        "    print(f\"\\nüìà Estat√≠sticas Gerais:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"  N√∫mero de √©pocas: {num_epochs}\")\n",
        "    print(f\"  Perda inicial (treino): {train_loss[0]:.4f}\")\n",
        "    print(f\"  Perda final (treino): {train_loss[-1]:.4f}\")\n",
        "    print(f\"  Perda inicial (valida√ß√£o): {val_loss[0]:.4f}\")\n",
        "    print(f\"  Perda final (valida√ß√£o): {val_loss[-1]:.4f}\")\n",
        "    print(f\"  Melhoria na perda: {(train_loss[0] - train_loss[-1])/train_loss[0]*100:.1f}%\")\n",
        "\n",
        "    print(f\"\\n  Precis√£o inicial (treino): {train_acc[0]:.2f}%\")\n",
        "    print(f\"  Precis√£o final (treino): {train_acc[-1]:.2f}%\")\n",
        "    print(f\"  Precis√£o inicial (valida√ß√£o): {val_acc[0]:.2f}%\")\n",
        "    print(f\"  Precis√£o final (valida√ß√£o): {val_acc[-1]:.2f}%\")\n",
        "    print(f\"  Melhoria na precis√£o: {train_acc[-1] - train_acc[0]:.2f}%\")\n",
        "\n",
        "    # Detectar overfitting\n",
        "    print(f\"\\nüîç An√°lise de Overfitting:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    gap_loss = train_loss[-1] - val_loss[-1]\n",
        "    gap_acc = train_acc[-1] - val_acc[-1]\n",
        "\n",
        "    print(f\"  Gap perda (treino - valida√ß√£o): {gap_loss:.4f}\")\n",
        "    print(f\"  Gap precis√£o (treino - valida√ß√£o): {gap_acc:.2f}%\")\n",
        "\n",
        "    if abs(gap_loss) < 0.05 and abs(gap_acc) < 2:\n",
        "        print(\"  ‚úÖ Sem sinais de overfitting significativo\")\n",
        "    elif abs(gap_loss) < 0.1 and abs(gap_acc) < 5:\n",
        "        print(\"  ‚ö†Ô∏è  Leve overfitting detectado\")\n",
        "    else:\n",
        "        print(\"  ‚ùå Overfitting significativo detectado\")\n",
        "\n",
        "    # Converg√™ncia\n",
        "    print(f\"\\nüéØ An√°lise de Converg√™ncia:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    if num_epochs >= 10:\n",
        "        last_5_loss_change = abs(val_loss[-1] - val_loss[-5])\n",
        "        last_5_acc_change = abs(val_acc[-1] - val_acc[-5])\n",
        "\n",
        "        print(f\"  Varia√ß√£o perda (√∫ltimas 5 √©pocas): {last_5_loss_change:.4f}\")\n",
        "        print(f\"  Varia√ß√£o precis√£o (√∫ltimas 5 √©pocas): {last_5_acc_change:.2f}%\")\n",
        "\n",
        "        if last_5_loss_change < 0.01 and last_5_acc_change < 0.5:\n",
        "            print(\"  ‚úÖ Modelo convergiu adequadamente\")\n",
        "        else:\n",
        "            print(\"  ‚ö†Ô∏è  Modelo ainda estava melhorando\")\n",
        "\n",
        "    # Melhor √©poca\n",
        "    best_epoch = np.argmin(val_loss)\n",
        "    print(f\"\\n‚≠ê Melhor √âpoca: {best_epoch + 1}\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"  Perda valida√ß√£o: {val_loss[best_epoch]:.4f}\")\n",
        "    print(f\"  Precis√£o valida√ß√£o: {val_acc[best_epoch]:.2f}%\")\n",
        "\n",
        "\n",
        "def plot_detailed_analysis(history, cm, save_path='detailed_analysis.png'):\n",
        "    \"\"\"\n",
        "    Cria visualiza√ß√£o completa da an√°lise\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "    # 1. Perda ao longo das √©pocas\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    ax1.plot(epochs, history['train_loss'], 'b-', label='Treino', linewidth=2)\n",
        "    ax1.plot(epochs, history['val_loss'], 'r--', label='Valida√ß√£o', linewidth=2)\n",
        "    ax1.set_xlabel('√âpocas')\n",
        "    ax1.set_ylabel('Perda')\n",
        "    ax1.set_title('Evolu√ß√£o da Perda', fontweight='bold')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Precis√£o ao longo das √©pocas\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    ax2.plot(epochs, history['train_acc'], 'b-', label='Treino', linewidth=2)\n",
        "    ax2.plot(epochs, history['val_acc'], 'r--', label='Valida√ß√£o', linewidth=2)\n",
        "    ax2.set_xlabel('√âpocas')\n",
        "    ax2.set_ylabel('Precis√£o (%)')\n",
        "    ax2.set_title('Evolu√ß√£o da Precis√£o', fontweight='bold')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Gap entre treino e valida√ß√£o\n",
        "    ax3 = fig.add_subplot(gs[0, 2])\n",
        "    gap_loss = [t - v for t, v in zip(history['train_loss'], history['val_loss'])]\n",
        "    ax3.plot(epochs, gap_loss, 'g-', linewidth=2)\n",
        "    ax3.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
        "    ax3.set_xlabel('√âpocas')\n",
        "    ax3.set_ylabel('Gap (Treino - Valida√ß√£o)')\n",
        "    ax3.set_title('An√°lise de Overfitting (Perda)', fontweight='bold')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Matriz de confus√£o normalizada\n",
        "    ax4 = fig.add_subplot(gs[1, :2])\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "    sns.heatmap(cm_normalized, annot=True, fmt='.1f', cmap='Blues',\n",
        "                xticklabels=['1', '2', '3', '4', '5'],\n",
        "                yticklabels=['1', '2', '3', '4', '5'],\n",
        "                cbar_kws={'label': 'Percentual (%)'},\n",
        "                ax=ax4, vmin=0, vmax=100)\n",
        "    ax4.set_xlabel('Predi√ß√£o')\n",
        "    ax4.set_ylabel('Real')\n",
        "    ax4.set_title('Matriz de Confus√£o Normalizada', fontweight='bold')\n",
        "\n",
        "    # 5. Precis√£o por classe\n",
        "    ax5 = fig.add_subplot(gs[1, 2])\n",
        "    diagonal = np.diag(cm_normalized)\n",
        "    classes = ['1', '2', '3', '4', '5']\n",
        "    colors = ['green' if x >= 98 else 'orange' if x >= 95 else 'red' for x in diagonal]\n",
        "    bars = ax5.bar(classes, diagonal, color=colors, alpha=0.7)\n",
        "    ax5.axhline(y=98, color='r', linestyle='--', alpha=0.5, label='Meta (98%)')\n",
        "    ax5.set_xlabel('D√≠gito')\n",
        "    ax5.set_ylabel('Precis√£o (%)')\n",
        "    ax5.set_title('Precis√£o por Classe', fontweight='bold')\n",
        "    ax5.legend()\n",
        "    ax5.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Adicionar valores nas barras\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "    # 6. Hist√≥rico de melhoria\n",
        "    ax6 = fig.add_subplot(gs[2, 0])\n",
        "    improvements_loss = []\n",
        "    for i in range(1, len(history['val_loss'])):\n",
        "        improvement = history['val_loss'][i-1] - history['val_loss'][i]\n",
        "        improvements_loss.append(improvement)\n",
        "\n",
        "    ax6.bar(range(2, len(history['val_loss']) + 1), improvements_loss,\n",
        "            color=['green' if x > 0 else 'red' for x in improvements_loss], alpha=0.7)\n",
        "    ax6.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "    ax6.set_xlabel('√âpoca')\n",
        "    ax6.set_ylabel('Melhoria na Perda')\n",
        "    ax6.set_title('Melhoria √âpoca a √âpoca', fontweight='bold')\n",
        "    ax6.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # 7. Learning curve\n",
        "    ax7 = fig.add_subplot(gs[2, 1:])\n",
        "    ax7.plot(epochs, history['train_loss'], 'b-', label='Perda Treino', linewidth=2)\n",
        "    ax7.plot(epochs, history['val_loss'], 'r--', label='Perda Valida√ß√£o', linewidth=2)\n",
        "    ax7_twin = ax7.twinx()\n",
        "    ax7_twin.plot(epochs, history['train_acc'], 'g-', label='Acc Treino',\n",
        "                  linewidth=2, alpha=0.5)\n",
        "    ax7_twin.plot(epochs, history['val_acc'], 'm--', label='Acc Valida√ß√£o',\n",
        "                  linewidth=2, alpha=0.5)\n",
        "\n",
        "    ax7.set_xlabel('√âpocas')\n",
        "    ax7.set_ylabel('Perda', color='b')\n",
        "    ax7_twin.set_ylabel('Precis√£o (%)', color='g')\n",
        "    ax7.set_title('Learning Curve Completa', fontweight='bold')\n",
        "    ax7.legend(loc='upper left')\n",
        "    ax7_twin.legend(loc='upper right')\n",
        "    ax7.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.suptitle('An√°lise Detalhada do Treinamento - Estrutura 4',\n",
        "                fontsize=16, fontweight='bold', y=0.995)\n",
        "\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"\\nüìä Visualiza√ß√£o detalhada salva em: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def generate_report(checkpoint, save_path='analysis_report.txt'):\n",
        "    \"\"\"\n",
        "    Gera relat√≥rio completo em texto\n",
        "    \"\"\"\n",
        "    with open(save_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"=\"*70 + \"\\n\")\n",
        "        f.write(\"RELAT√ìRIO DE AN√ÅLISE - CNN ESTRUTURA 4\\n\")\n",
        "        f.write(\"Classifica√ß√£o de D√≠gitos 1-5 (EMNIST)\\n\")\n",
        "        f.write(\"Baseado em: Silva Filho et al. (2022)\\n\")\n",
        "        f.write(\"=\"*70 + \"\\n\\n\")\n",
        "\n",
        "        # M√©tricas principais\n",
        "        f.write(\"M√âTRICAS PRINCIPAIS\\n\")\n",
        "        f.write(\"-\"*70 + \"\\n\")\n",
        "        f.write(f\"Precis√£o no teste: {checkpoint['accuracy']:.2f}%\\n\")\n",
        "\n",
        "        cm = checkpoint['confusion_matrix']\n",
        "        history = checkpoint['history']\n",
        "\n",
        "        f.write(f\"Perda final (treino): {history['train_loss'][-1]:.4f}\\n\")\n",
        "        f.write(f\"Perda final (valida√ß√£o): {history['val_loss'][-1]:.4f}\\n\")\n",
        "        f.write(f\"N√∫mero de √©pocas: {len(history['train_loss'])}\\n\\n\")\n",
        "\n",
        "        # An√°lise por classe\n",
        "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "        diagonal = np.diag(cm_normalized)\n",
        "\n",
        "        f.write(\"PRECIS√ÉO POR CLASSE\\n\")\n",
        "        f.write(\"-\"*70 + \"\\n\")\n",
        "        for i, acc in enumerate(diagonal, 1):\n",
        "            f.write(f\"D√≠gito {i}: {acc:.2f}%\\n\")\n",
        "\n",
        "        f.write(f\"\\nMelhor classe: D√≠gito {np.argmax(diagonal)+1} ({diagonal.max():.2f}%)\\n\")\n",
        "        f.write(f\"Pior classe: D√≠gito {np.argmin(diagonal)+1} ({diagonal.min():.2f}%)\\n\\n\")\n",
        "\n",
        "        # Compara√ß√£o com artigo\n",
        "        f.write(\"COMPARA√á√ÉO COM O ARTIGO\\n\")\n",
        "        f.write(\"-\"*70 + \"\\n\")\n",
        "        f.write(\"Artigo (Estrutura 4, d√≠gitos 0-9): 98,84%\\n\")\n",
        "        f.write(f\"Obtido (d√≠gitos 1-5): {checkpoint['accuracy']:.2f}%\\n\")\n",
        "        f.write(f\"Diferen√ßa: {checkpoint['accuracy'] - 98.84:+.2f}%\\n\\n\")\n",
        "\n",
        "        f.write(\"Artigo (perda): 0,064\\n\")\n",
        "        f.write(f\"Obtido (perda valida√ß√£o): {history['val_loss'][-1]:.4f}\\n\")\n",
        "        f.write(f\"Diferen√ßa: {history['val_loss'][-1] - 0.064:+.4f}\\n\\n\")\n",
        "\n",
        "        # Conclus√£o\n",
        "        f.write(\"CONCLUS√ÉO\\n\")\n",
        "        f.write(\"-\"*70 + \"\\n\")\n",
        "        if checkpoint['accuracy'] >= 98.0:\n",
        "            f.write(\"‚úÖ Resultados EXCELENTES! Compar√°veis ao artigo original.\\n\")\n",
        "            f.write(\"A arquitetura Estrutura 4 demonstrou alta efic√°cia na\\n\")\n",
        "            f.write(\"classifica√ß√£o dos d√≠gitos 1-5 do EMNIST.\\n\")\n",
        "        elif checkpoint['accuracy'] >= 97.0:\n",
        "            f.write(\"‚úÖ Resultados MUITO BONS! Pr√≥ximos ao artigo original.\\n\")\n",
        "        else:\n",
        "            f.write(\"‚ö†Ô∏è Resultados podem ser melhorados.\\n\")\n",
        "\n",
        "    print(f\"\\nüìù Relat√≥rio completo salvo em: {save_path}\")"
      ],
      "metadata": {
        "id": "7rwgybwDvayI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Fun√ß√£o principal para an√°lise completa\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"AN√ÅLISE DETALHADA DOS RESULTADOS\")\n",
        "    print(\"CNN Estrutura 4 - D√≠gitos 1-5 (EMNIST)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    try:\n",
        "        # Carregar resultados\n",
        "        print(\"\\nüìÇ Carregando resultados...\")\n",
        "        checkpoint = load_results()\n",
        "\n",
        "        accuracy = checkpoint['accuracy']\n",
        "        cm = checkpoint['confusion_matrix']\n",
        "        history = checkpoint['history']\n",
        "\n",
        "        # Calcular perda final\n",
        "        final_loss = history['val_loss'][-1]\n",
        "\n",
        "        # An√°lise da matriz de confus√£o\n",
        "        cm_norm, diagonal, confusions = analyze_confusion_matrix(cm)\n",
        "\n",
        "        # Compara√ß√£o com artigo\n",
        "        comparison = compare_with_article(accuracy, final_loss, cm)\n",
        "\n",
        "        # An√°lise do hist√≥rico\n",
        "        analyze_training_history(history)\n",
        "\n",
        "        # Gerar visualiza√ß√µes\n",
        "        print(\"\\nüìä Gerando visualiza√ß√µes...\")\n",
        "        plot_detailed_analysis(history, cm)\n",
        "\n",
        "        # Gerar relat√≥rio\n",
        "        print(\"\\nüìù Gerando relat√≥rio...\")\n",
        "        generate_report(checkpoint)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"AN√ÅLISE CONCLU√çDA COM SUCESSO!\")\n",
        "        print(\"=\"*70)\n",
        "        print(\"\\nArquivos gerados:\")\n",
        "        print(\"  - detailed_analysis.png (visualiza√ß√µes)\")\n",
        "        print(\"  - analysis_report.txt (relat√≥rio)\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\n‚ùå Erro: Arquivo do modelo n√£o encontrado!\")\n",
        "        print(\"Execute primeiro: python cnn_emnist_digits.py\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Erro durante a an√°lise: {str(e)}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGx9Kvs-vp7t",
        "outputId": "c263b023-5fa0-4a08-e36e-b22944cb00a5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "AN√ÅLISE DETALHADA DOS RESULTADOS\n",
            "CNN Estrutura 4 - D√≠gitos 1-5 (EMNIST)\n",
            "======================================================================\n",
            "\n",
            "üìÇ Carregando resultados...\n",
            "\n",
            "======================================================================\n",
            "AN√ÅLISE DA MATRIZ DE CONFUS√ÉO\n",
            "======================================================================\n",
            "\n",
            "üìä Taxa de Acerto por Classe:\n",
            "--------------------------------------------------\n",
            "  D√≠gito 1: 99.92%\n",
            "  D√≠gito 2: 99.72%\n",
            "  D√≠gito 3: 99.43%\n",
            "  D√≠gito 4: 99.84%\n",
            "  D√≠gito 5: 99.86%\n",
            "\n",
            "‚úÖ Melhor classe: D√≠gito 1 (99.92%)\n",
            "‚ö†Ô∏è  Pior classe: D√≠gito 3 (99.43%)\n",
            "\n",
            "üîÄ Principais Confus√µes:\n",
            "--------------------------------------------------\n",
            "  1. D√≠gito 3 confundido com 5: 0.37%\n",
            "  2. D√≠gito 2 confundido com 3: 0.17%\n",
            "  3. D√≠gito 5 confundido com 3: 0.14%\n",
            "  4. D√≠gito 3 confundido com 1: 0.09%\n",
            "  5. D√≠gito 2 confundido com 1: 0.08%\n",
            "\n",
            "‚ùå Taxa m√°xima de erro: 0.37%\n",
            "\n",
            "======================================================================\n",
            "COMPARA√á√ÉO COM OS RESULTADOS DO ARTIGO\n",
            "======================================================================\n",
            "\n",
            "üìÑ Resultados do Artigo (Estrutura 4, d√≠gitos 0-9):\n",
            "--------------------------------------------------\n",
            "  Precis√£o geral: 98.84%\n",
            "  Perda: 0.064\n",
            "  Taxa m√≠nima por classe: 98.00%\n",
            "  Erro m√°ximo entre classes: 0.90%\n",
            "  √âpocas de treinamento: 24\n",
            "  Tempo de treinamento: ~150 segundos\n",
            "\n",
            "üî¨ Resultados Obtidos (d√≠gitos 1-5):\n",
            "--------------------------------------------------\n",
            "  Precis√£o geral: 99.76%\n",
            "  Perda: 0.011\n",
            "  Taxa m√≠nima por classe: 99.43%\n",
            "  Erro m√°ximo entre classes: 0.37%\n",
            "\n",
            "üìä An√°lise Comparativa:\n",
            "--------------------------------------------------\n",
            "  Diferen√ßa na precis√£o: +0.92% ‚úÖ\n",
            "  Diferen√ßa na perda: -0.053 ‚úÖ\n",
            "  Diferen√ßa taxa m√≠n. classe: +1.43% ‚úÖ\n",
            "  Diferen√ßa erro m√°ximo: -0.53% ‚úÖ\n",
            "\n",
            "üí° Conclus√£o:\n",
            "--------------------------------------------------\n",
            "  ‚úÖ Resultados EXCELENTES! Compar√°veis ao artigo.\n",
            "\n",
            "======================================================================\n",
            "AN√ÅLISE DO HIST√ìRICO DE TREINAMENTO\n",
            "======================================================================\n",
            "\n",
            "üìà Estat√≠sticas Gerais:\n",
            "--------------------------------------------------\n",
            "  N√∫mero de √©pocas: 14\n",
            "  Perda inicial (treino): 0.0540\n",
            "  Perda final (treino): 0.0029\n",
            "  Perda inicial (valida√ß√£o): 0.0159\n",
            "  Perda final (valida√ß√£o): 0.0112\n",
            "  Melhoria na perda: 94.6%\n",
            "\n",
            "  Precis√£o inicial (treino): 98.25%\n",
            "  Precis√£o final (treino): 99.91%\n",
            "  Precis√£o inicial (valida√ß√£o): 99.53%\n",
            "  Precis√£o final (valida√ß√£o): 99.72%\n",
            "  Melhoria na precis√£o: 1.66%\n",
            "\n",
            "üîç An√°lise de Overfitting:\n",
            "--------------------------------------------------\n",
            "  Gap perda (treino - valida√ß√£o): -0.0082\n",
            "  Gap precis√£o (treino - valida√ß√£o): 0.19%\n",
            "  ‚úÖ Sem sinais de overfitting significativo\n",
            "\n",
            "üéØ An√°lise de Converg√™ncia:\n",
            "--------------------------------------------------\n",
            "  Varia√ß√£o perda (√∫ltimas 5 √©pocas): 0.0001\n",
            "  Varia√ß√£o precis√£o (√∫ltimas 5 √©pocas): 0.01%\n",
            "  ‚úÖ Modelo convergiu adequadamente\n",
            "\n",
            "‚≠ê Melhor √âpoca: 9\n",
            "--------------------------------------------------\n",
            "  Perda valida√ß√£o: 0.0080\n",
            "  Precis√£o valida√ß√£o: 99.77%\n",
            "\n",
            "üìä Gerando visualiza√ß√µes...\n",
            "\n",
            "üìä Visualiza√ß√£o detalhada salva em: detailed_analysis.png\n",
            "\n",
            "üìù Gerando relat√≥rio...\n",
            "\n",
            "üìù Relat√≥rio completo salvo em: analysis_report.txt\n",
            "\n",
            "======================================================================\n",
            "AN√ÅLISE CONCLU√çDA COM SUCESSO!\n",
            "======================================================================\n",
            "\n",
            "Arquivos gerados:\n",
            "  - detailed_analysis.png (visualiza√ß√µes)\n",
            "  - analysis_report.txt (relat√≥rio)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "3WAqszFExoK-",
        "outputId": "9a3cfa44-d134-4177-aed8-1834d83aa4d6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'torch.nn' has no attribute 'torchsummary'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2983288419.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorchsummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'torchsummary'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dd9dbf6"
      },
      "source": [
        "### Sum√°rio da Arquitetura do Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9035479"
      },
      "source": [
        "Esta se√ß√£o apresenta a arquitetura da CNN 'Estrutura 4' implementada, conforme especificado no artigo, e resume a contagem de par√¢metros do modelo. Isso √© √∫til para verificar se a estrutura do modelo est√° correta e para entender a complexidade do modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c3152ef",
        "outputId": "ec5a2554-1ad3-419e-b364-7ad7a4d7f022"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Instantiate the model (assuming CNNEstrutura4 class is already defined and executed)\n",
        "model_summary = CNNEstrutura4(num_classes=5)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model_summary.parameters())\n",
        "trainable_params = sum(p.numel() for p in model_summary.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RESUMO DA ARQUITETURA DO MODELO\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nPar√¢metros totais: {total_params:,}\")\n",
        "print(f\"Par√¢metros trein√°veis: {trainable_params:,}\")\n",
        "\n",
        "print(\"\\nArquitetura do modelo:\")\n",
        "print(model_summary)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "RESUMO DA ARQUITETURA DO MODELO\n",
            "======================================================================\n",
            "\n",
            "Par√¢metros totais: 93,957\n",
            "Par√¢metros trein√°veis: 93,957\n",
            "\n",
            "Arquitetura do modelo:\n",
            "CNNEstrutura4(\n",
            "  (conv_block1): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv_block2): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv_block3): Sequential(\n",
            "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv_block4): Sequential(\n",
            "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n"
          ]
        }
      ]
    }
  ]
}