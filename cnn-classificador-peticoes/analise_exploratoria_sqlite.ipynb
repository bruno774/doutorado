{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An√°lise Explorat√≥ria de Dados SQLite para Treinamento de Redes Neurais\n",
    "\n",
    "Este notebook realiza uma an√°lise explorat√≥ria completa de dados armazenados em arquivos SQLite3, preparando-os para treinamento de modelos de aprendizado profundo.\n",
    "\n",
    "## Objetivos:\n",
    "- Conectar e explorar bases de dados SQLite3\n",
    "- Analisar vari√°veis num√©ricas, categ√≥ricas e campos de texto\n",
    "- Identificar problemas de qualidade de dados\n",
    "- Preparar dados para modelos de redes neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importa√ß√£o de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configura√ß√£o e Conex√£o ao Banco de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar o caminho para o arquivo SQLite\n",
    "DB_PATH = r\"c:\\Users\\bruno.silva\\Codes\\cnn-classificador-peticoes\\seu_arquivo.db\"  # Altere para o caminho do seu arquivo\n",
    "\n",
    "# Verificar se o arquivo existe\n",
    "if not Path(DB_PATH).exists():\n",
    "    print(f\"AVISO: Arquivo n√£o encontrado em {DB_PATH}\")\n",
    "    print(\"Por favor, atualize o caminho do arquivo SQLite na vari√°vel DB_PATH\")\n",
    "else:\n",
    "    print(f\"Arquivo encontrado: {DB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para conectar ao banco de dados\n",
    "def connect_to_db(db_path):\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        print(f\"Conex√£o estabelecida com sucesso!\")\n",
    "        return conn\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Erro ao conectar: {e}\")\n",
    "        return None\n",
    "\n",
    "conn = connect_to_db(DB_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explora√ß√£o da Estrutura do Banco de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar todas as tabelas dispon√≠veis\n",
    "def list_tables(conn):\n",
    "    query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    tables = pd.read_sql_query(query, conn)\n",
    "    return tables\n",
    "\n",
    "if conn:\n",
    "    tables_df = list_tables(conn)\n",
    "    print(f\"Tabelas encontradas: {len(tables_df)}\")\n",
    "    print(\"\\nLista de tabelas:\")\n",
    "    display(tables_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorar schema de cada tabela\n",
    "def get_table_info(conn, table_name):\n",
    "    query = f\"PRAGMA table_info({table_name});\"\n",
    "    return pd.read_sql_query(query, conn)\n",
    "\n",
    "if conn and not tables_df.empty:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ESTRUTURA DAS TABELAS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for table_name in tables_df['name']:\n",
    "        print(f\"\\nüìä Tabela: {table_name}\")\n",
    "        schema = get_table_info(conn, table_name)\n",
    "        display(schema)\n",
    "        \n",
    "        # Contar registros\n",
    "        count_query = f\"SELECT COUNT(*) as total FROM {table_name}\"\n",
    "        count = pd.read_sql_query(count_query, conn)['total'][0]\n",
    "        print(f\"Total de registros: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Carregamento e Visualiza√ß√£o Inicial dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar a tabela principal para an√°lise\n",
    "# AJUSTE ESTA VARI√ÅVEL COM O NOME DA SUA TABELA\n",
    "TABLE_NAME = tables_df['name'][0] if not tables_df.empty else \"sua_tabela\"\n",
    "\n",
    "print(f\"Analisando a tabela: {TABLE_NAME}\")\n",
    "print(\"\\nSe desejar analisar outra tabela, altere a vari√°vel TABLE_NAME acima.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados da tabela\n",
    "if conn:\n",
    "    query = f\"SELECT * FROM {TABLE_NAME}\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    print(f\"Dados carregados: {df.shape[0]:,} linhas x {df.shape[1]} colunas\")\n",
    "    print(\"\\nPrimeiras linhas:\")\n",
    "    display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informa√ß√µes gerais do dataset\n",
    "print(\"INFORMA√á√ïES GERAIS DO DATASET\")\n",
    "print(\"=\"*80)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. An√°lise de Qualidade dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de valores ausentes\n",
    "def analyze_missing_values(df):\n",
    "    missing = pd.DataFrame({\n",
    "        'Coluna': df.columns,\n",
    "        'Total_Missing': df.isnull().sum(),\n",
    "        'Percentual_Missing': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "        'Tipo_Dado': df.dtypes\n",
    "    })\n",
    "    missing = missing[missing['Total_Missing'] > 0].sort_values('Total_Missing', ascending=False)\n",
    "    return missing\n",
    "\n",
    "missing_analysis = analyze_missing_values(df)\n",
    "\n",
    "if not missing_analysis.empty:\n",
    "    print(\"\\n‚ö†Ô∏è VALORES AUSENTES DETECTADOS\")\n",
    "    print(\"=\"*80)\n",
    "    display(missing_analysis)\n",
    "    \n",
    "    # Visualizar valores ausentes\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(missing_analysis['Coluna'], missing_analysis['Percentual_Missing'])\n",
    "    plt.xlabel('Percentual de Valores Ausentes (%)')\n",
    "    plt.title('An√°lise de Dados Ausentes por Coluna')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n‚úÖ Nenhum valor ausente detectado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de duplicatas\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nTotal de registros duplicados: {duplicates:,} ({duplicates/len(df)*100:.2f}%)\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"\\nExemplo de registros duplicados:\")\n",
    "    display(df[df.duplicated(keep=False)].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Classifica√ß√£o Autom√°tica de Vari√°veis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar tipos de vari√°veis\n",
    "def classify_variables(df, text_threshold=50):\n",
    "    numeric_vars = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    categorical_vars = []\n",
    "    text_vars = []\n",
    "    \n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        unique_ratio = df[col].nunique() / len(df)\n",
    "        avg_length = df[col].astype(str).str.len().mean()\n",
    "        \n",
    "        if avg_length > text_threshold or unique_ratio > 0.5:\n",
    "            text_vars.append(col)\n",
    "        else:\n",
    "            categorical_vars.append(col)\n",
    "    \n",
    "    return {\n",
    "        'numericas': numeric_vars,\n",
    "        'categoricas': categorical_vars,\n",
    "        'texto': text_vars\n",
    "    }\n",
    "\n",
    "var_types = classify_variables(df)\n",
    "\n",
    "print(\"\\nüìä CLASSIFICA√á√ÉO DE VARI√ÅVEIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚úì Vari√°veis Num√©ricas ({len(var_types['numericas'])}):\", var_types['numericas'])\n",
    "print(f\"\\n‚úì Vari√°veis Categ√≥ricas ({len(var_types['categoricas'])}):\", var_types['categoricas'])\n",
    "print(f\"\\n‚úì Campos de Texto ({len(var_types['texto'])}):\", var_types['texto'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. An√°lise de Vari√°veis Num√©ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estat√≠sticas descritivas das vari√°veis num√©ricas\n",
    "if var_types['numericas']:\n",
    "    print(\"\\nüìà ESTAT√çSTICAS DESCRITIVAS - VARI√ÅVEIS NUM√âRICAS\")\n",
    "    print(\"=\"*80)\n",
    "    display(df[var_types['numericas']].describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribui√ß√µes das vari√°veis num√©ricas\n",
    "if var_types['numericas']:\n",
    "    n_cols = min(3, len(var_types['numericas']))\n",
    "    n_rows = (len(var_types['numericas']) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    axes = axes.flatten() if len(var_types['numericas']) > 1 else [axes]\n",
    "    \n",
    "    for idx, col in enumerate(var_types['numericas']):\n",
    "        df[col].hist(bins=30, ax=axes[idx], edgecolor='black', alpha=0.7)\n",
    "        axes[idx].set_title(f'Distribui√ß√£o: {col}')\n",
    "        axes[idx].set_xlabel(col)\n",
    "        axes[idx].set_ylabel('Frequ√™ncia')\n",
    "    \n",
    "    # Remover subplots vazios\n",
    "    for idx in range(len(var_types['numericas']), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots para detectar outliers\n",
    "if var_types['numericas']:\n",
    "    n_cols = min(3, len(var_types['numericas']))\n",
    "    n_rows = (len(var_types['numericas']) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    axes = axes.flatten() if len(var_types['numericas']) > 1 else [axes]\n",
    "    \n",
    "    for idx, col in enumerate(var_types['numericas']):\n",
    "        df.boxplot(column=col, ax=axes[idx])\n",
    "        axes[idx].set_title(f'Boxplot: {col}')\n",
    "        axes[idx].set_ylabel(col)\n",
    "    \n",
    "    # Remover subplots vazios\n",
    "    for idx in range(len(var_types['numericas']), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correla√ß√£o\n",
    "if len(var_types['numericas']) > 1:\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    correlation_matrix = df[var_types['numericas']].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, linewidths=1, fmt='.2f')\n",
    "    plt.title('Matriz de Correla√ß√£o - Vari√°veis Num√©ricas')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Correla√ß√µes mais fortes\n",
    "    print(\"\\nüîó CORRELA√á√ïES MAIS FORTES (|r| > 0.5):\")\n",
    "    print(\"=\"*80)\n",
    "    corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > 0.5:\n",
    "                corr_pairs.append({\n",
    "                    'Var1': correlation_matrix.columns[i],\n",
    "                    'Var2': correlation_matrix.columns[j],\n",
    "                    'Correla√ß√£o': correlation_matrix.iloc[i, j]\n",
    "                })\n",
    "    \n",
    "    if corr_pairs:\n",
    "        corr_df = pd.DataFrame(corr_pairs).sort_values('Correla√ß√£o', ascending=False, key=abs)\n",
    "        display(corr_df)\n",
    "    else:\n",
    "        print(\"Nenhuma correla√ß√£o forte detectada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. An√°lise de Vari√°veis Categ√≥ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de cardinalidade das vari√°veis categ√≥ricas\n",
    "if var_types['categoricas']:\n",
    "    print(\"\\nüìã AN√ÅLISE DE VARI√ÅVEIS CATEG√ìRICAS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    categorical_summary = []\n",
    "    for col in var_types['categoricas']:\n",
    "        categorical_summary.append({\n",
    "            'Vari√°vel': col,\n",
    "            'Valores_√önicos': df[col].nunique(),\n",
    "            'Valor_Mais_Frequente': df[col].mode()[0] if not df[col].mode().empty else None,\n",
    "            'Frequ√™ncia_M√°xima': df[col].value_counts().iloc[0] if len(df[col]) > 0 else 0,\n",
    "            'Percentual_M√°ximo': f\"{df[col].value_counts(normalize=True).iloc[0]*100:.2f}%\" if len(df[col]) > 0 else \"0%\"\n",
    "        })\n",
    "    \n",
    "    cat_summary_df = pd.DataFrame(categorical_summary)\n",
    "    display(cat_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribui√ß√£o de frequ√™ncias das vari√°veis categ√≥ricas\n",
    "if var_types['categoricas']:\n",
    "    for col in var_types['categoricas']:\n",
    "        print(f\"\\nüìä Distribui√ß√£o: {col}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        value_counts = df[col].value_counts()\n",
    "        \n",
    "        # Mostrar top 10 valores\n",
    "        print(f\"\\nTop 10 valores mais frequentes:\")\n",
    "        display(pd.DataFrame({\n",
    "            'Valor': value_counts.head(10).index,\n",
    "            'Frequ√™ncia': value_counts.head(10).values,\n",
    "            'Percentual': (value_counts.head(10).values / len(df) * 100).round(2)\n",
    "        }))\n",
    "        \n",
    "        # Gr√°fico de barras (limitado a top 15 para legibilidade)\n",
    "        if len(value_counts) <= 15:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            value_counts.plot(kind='bar', edgecolor='black', alpha=0.7)\n",
    "            plt.title(f'Distribui√ß√£o de Frequ√™ncias: {col}')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Frequ√™ncia')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            value_counts.head(15).plot(kind='bar', edgecolor='black', alpha=0.7)\n",
    "            plt.title(f'Top 15 Valores - {col}')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Frequ√™ncia')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. An√°lise de Campos de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de campos de texto\n",
    "if var_types['texto']:\n",
    "    print(\"\\nüìù AN√ÅLISE DE CAMPOS DE TEXTO\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    text_summary = []\n",
    "    for col in var_types['texto']:\n",
    "        text_lengths = df[col].astype(str).str.len()\n",
    "        text_summary.append({\n",
    "            'Campo': col,\n",
    "            'Valores_√önicos': df[col].nunique(),\n",
    "            'Comprimento_M√©dio': text_lengths.mean().round(2),\n",
    "            'Comprimento_Min': text_lengths.min(),\n",
    "            'Comprimento_Max': text_lengths.max(),\n",
    "            'Comprimento_Mediana': text_lengths.median()\n",
    "        })\n",
    "    \n",
    "    text_summary_df = pd.DataFrame(text_summary)\n",
    "    display(text_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribui√ß√£o de comprimento de texto\n",
    "if var_types['texto']:\n",
    "    for col in var_types['texto']:\n",
    "        print(f\"\\nüìä Distribui√ß√£o de Comprimento: {col}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        text_lengths = df[col].astype(str).str.len()\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(text_lengths, bins=30, edgecolor='black', alpha=0.7)\n",
    "        plt.xlabel('Comprimento do Texto')\n",
    "        plt.ylabel('Frequ√™ncia')\n",
    "        plt.title(f'Histograma de Comprimento: {col}')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.boxplot(text_lengths)\n",
    "        plt.ylabel('Comprimento do Texto')\n",
    "        plt.title(f'Boxplot de Comprimento: {col}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Exemplos de textos\n",
    "        print(f\"\\nExemplos de textos (primeiros 5 valores √∫nicos):\")\n",
    "        for idx, text in enumerate(df[col].dropna().unique()[:5], 1):\n",
    "            preview = str(text)[:100] + \"...\" if len(str(text)) > 100 else str(text)\n",
    "            print(f\"{idx}. {preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. An√°lise de Palavras mais Frequentes (Campos de Texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de palavras mais frequentes\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "if var_types['texto']:\n",
    "    for col in var_types['texto']:\n",
    "        print(f\"\\nüî§ Palavras Mais Frequentes: {col}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Concatenar todos os textos\n",
    "        all_text = ' '.join(df[col].astype(str).values)\n",
    "        \n",
    "        # Extrair palavras (apenas letras, m√≠nimo 3 caracteres)\n",
    "        words = re.findall(r'\\b[a-z√°√†√¢√£√©√®√™√≠√Ø√≥√¥√µ√∂√∫√ß√±]{3,}\\b', all_text.lower())\n",
    "        \n",
    "        # Contar palavras\n",
    "        word_counts = Counter(words)\n",
    "        \n",
    "        # Top 20 palavras\n",
    "        top_words = pd.DataFrame(word_counts.most_common(20), \n",
    "                                columns=['Palavra', 'Frequ√™ncia'])\n",
    "        \n",
    "        display(top_words)\n",
    "        \n",
    "        # Gr√°fico de barras\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.barh(top_words['Palavra'][::-1], top_words['Frequ√™ncia'][::-1], alpha=0.7)\n",
    "        plt.xlabel('Frequ√™ncia')\n",
    "        plt.title(f'Top 20 Palavras Mais Frequentes: {col}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Relat√≥rio de Prepara√ß√£o para Redes Neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar relat√≥rio de prepara√ß√£o\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RELAT√ìRIO DE PREPARA√á√ÉO PARA TREINAMENTO DE REDES NEURAIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä RESUMO DO DATASET\")\n",
    "print(f\"  ‚Ä¢ Total de registros: {len(df):,}\")\n",
    "print(f\"  ‚Ä¢ Total de features: {df.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Duplicatas: {duplicates:,}\")\n",
    "\n",
    "print(f\"\\nüî¢ VARI√ÅVEIS NUM√âRICAS ({len(var_types['numericas'])})\")\n",
    "if var_types['numericas']:\n",
    "    for var in var_types['numericas']:\n",
    "        print(f\"  ‚Ä¢ {var}\")\n",
    "    print(\"\\n  Recomenda√ß√µes:\")\n",
    "    print(\"    - Normalizar/Padronizar antes do treinamento\")\n",
    "    print(\"    - Verificar outliers e decidir tratamento\")\n",
    "    print(\"    - Analisar correla√ß√µes para feature engineering\")\n",
    "\n",
    "print(f\"\\nüìã VARI√ÅVEIS CATEG√ìRICAS ({len(var_types['categoricas'])})\")\n",
    "if var_types['categoricas']:\n",
    "    for var in var_types['categoricas']:\n",
    "        n_unique = df[var].nunique()\n",
    "        print(f\"  ‚Ä¢ {var} ({n_unique} categorias √∫nicas)\")\n",
    "    print(\"\\n  Recomenda√ß√µes:\")\n",
    "    print(\"    - One-Hot Encoding para baixa cardinalidade (<10 categorias)\")\n",
    "    print(\"    - Label Encoding ou Embedding para alta cardinalidade\")\n",
    "    print(\"    - Considerar Target Encoding se houver vari√°vel alvo\")\n",
    "\n",
    "print(f\"\\nüìù CAMPOS DE TEXTO ({len(var_types['texto'])})\")\n",
    "if var_types['texto']:\n",
    "    for var in var_types['texto']:\n",
    "        print(f\"  ‚Ä¢ {var}\")\n",
    "    print(\"\\n  Recomenda√ß√µes:\")\n",
    "    print(\"    - TF-IDF ou Word Embeddings (Word2Vec, GloVe)\")\n",
    "    print(\"    - BERT/Transformers para contexto sem√¢ntico\")\n",
    "    print(\"    - Pr√©-processamento: remo√ß√£o de stopwords, stemming/lemmatization\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è PROBLEMAS IDENTIFICADOS\")\n",
    "issues = []\n",
    "if not missing_analysis.empty:\n",
    "    issues.append(f\"  ‚Ä¢ Valores ausentes em {len(missing_analysis)} colunas\")\n",
    "if duplicates > 0:\n",
    "    issues.append(f\"  ‚Ä¢ {duplicates:,} registros duplicados\")\n",
    "\n",
    "if issues:\n",
    "    for issue in issues:\n",
    "        print(issue)\n",
    "else:\n",
    "    print(\"  ‚úÖ Nenhum problema cr√≠tico detectado\")\n",
    "\n",
    "print(f\"\\nüéØ PR√ìXIMOS PASSOS\")\n",
    "print(\"  1. Tratamento de valores ausentes (imputa√ß√£o ou remo√ß√£o)\")\n",
    "print(\"  2. Codifica√ß√£o de vari√°veis categ√≥ricas\")\n",
    "print(\"  3. Processamento de texto (tokeniza√ß√£o, vetoriza√ß√£o)\")\n",
    "print(\"  4. Normaliza√ß√£o de features num√©ricas\")\n",
    "print(\"  5. Split train/validation/test\")\n",
    "print(\"  6. Feature engineering se necess√°rio\")\n",
    "print(\"  7. Defini√ß√£o da arquitetura da rede neural\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Exportar Dados para An√°lise Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar dados em CSV para an√°lise posterior\n",
    "output_path = r\"c:\\Users\\bruno.silva\\Codes\\cnn-classificador-peticoes\\dados_exportados.csv\"\n",
    "df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "print(f\"\\n‚úÖ Dados exportados com sucesso para: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar relat√≥rio de tipos de vari√°veis\n",
    "import json\n",
    "\n",
    "report = {\n",
    "    'dataset_info': {\n",
    "        'total_rows': len(df),\n",
    "        'total_columns': df.shape[1],\n",
    "        'duplicates': int(duplicates)\n",
    "    },\n",
    "    'variable_types': var_types,\n",
    "    'missing_values': missing_analysis.to_dict('records') if not missing_analysis.empty else []\n",
    "}\n",
    "\n",
    "report_path = r\"c:\\Users\\bruno.silva\\Codes\\cnn-classificador-peticoes\\analise_relatorio.json\"\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Relat√≥rio salvo em: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Fechar Conex√£o com o Banco de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fechar conex√£o\n",
    "if conn:\n",
    "    conn.close()\n",
    "    print(\"\\n‚úÖ Conex√£o com o banco de dados fechada com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclus√£o\n",
    "\n",
    "Este notebook realizou uma an√°lise explorat√≥ria completa dos dados SQLite3, incluindo:\n",
    "\n",
    "- ‚úÖ Conex√£o e explora√ß√£o da estrutura do banco\n",
    "- ‚úÖ An√°lise de qualidade dos dados (valores ausentes, duplicatas)\n",
    "- ‚úÖ Classifica√ß√£o autom√°tica de vari√°veis (num√©ricas, categ√≥ricas, texto)\n",
    "- ‚úÖ An√°lise estat√≠stica detalhada de cada tipo de vari√°vel\n",
    "- ‚úÖ Visualiza√ß√µes para melhor compreens√£o dos dados\n",
    "- ‚úÖ Recomenda√ß√µes para pr√©-processamento e treinamento de redes neurais\n",
    "\n",
    "Os dados est√£o prontos para as pr√≥ximas etapas de prepara√ß√£o e modelagem!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
