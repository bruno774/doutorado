{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c91e6e",
   "metadata": {},
   "source": [
    "## 1. Importa√ß√£o de Bibliotecas Essenciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3fa26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import warnings\n",
    "import time\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# Processamento de texto\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# M√©tricas e avalia√ß√£o\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configura√ß√µes\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Download NLTK resources\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Configurar device (GPU se dispon√≠vel)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úì Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úì CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "print(f\"‚úì PyTorch vers√£o: {torch.__version__}\")\n",
    "print(f\"‚úì NumPy vers√£o: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95af3134",
   "metadata": {},
   "source": [
    "## 2. Carregar e Gerar Dataset de Grande Escala (80k amostras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e197e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par√¢metros\n",
    "DB_PATH = \"/gdrive/MyDrive/Colab Notebooks/JFRN/split_dados_01.sqlite3\"\n",
    "TABLE_NAME = \"peticoes\"\n",
    "DATASET_SIZE = 80000\n",
    "\n",
    "print(f\"Carregando/Gerando dataset com {DATASET_SIZE:,} amostras...\")\n",
    "\n",
    "# Tentar carregar do SQLite\n",
    "try:\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    query = f\"SELECT * FROM {TABLE_NAME} LIMIT {DATASET_SIZE}\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    print(f\"‚úì Dados carregados do SQLite: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Erro ao carregar SQLite: {e}\")\n",
    "    print(f\"Gerando dataset sint√©tico com {DATASET_SIZE:,} amostras...\\n\")\n",
    "    \n",
    "    # Corpus variado de peti√ß√µes jur√≠dicas\n",
    "    corpus_templates = [\n",
    "        # Peti√ß√µes deferidas (linguagem favor√°vel)\n",
    "        \"Fundamentado em jurisprud√™ncia consolidada, solicita-se deferimento integral do pedido\",\n",
    "        \"Documenta√ß√£o completa e direito inequ√≠voco justificam a concess√£o pleiteada\",\n",
    "        \"Precedentes do STJ e STF amplaram o reconhecimento desta modalidade de direito\",\n",
    "        \"Ampla documenta√ß√£o apresentada comprova inequivocamente o direito invocado\",\n",
    "        \"Recurso extraordin√°rio com repercuss√£o geral reconhecida sobre mat√©ria j√° pacificada\",\n",
    "        \n",
    "        # Peti√ß√µes indeferidas (linguagem desfavor√°vel)\n",
    "        \"Faltam elementos essenciais √† constitui√ß√£o da rela√ß√£o jur√≠dica alegada\",\n",
    "        \"Prescri√ß√£o consumada extingue o direito de a√ß√£o pelo lapso temporal\",\n",
    "        \"Lacks documentary evidence and legal grounds for the claim presented\",\n",
    "        \"Recurso manifestamente infundado contradiz jurisprud√™ncia consolidada\",\n",
    "        \"Car√™ncia de legitimidade ativa e passiva invalida a demanda\",\n",
    "        \n",
    "        # Peti√ß√µes parcialmente deferidas (linguagem mista)\n",
    "        \"Parcialmente fundado o recurso, reconhecendo-se apenas parte da pretens√£o\",\n",
    "        \"Parte do pleito merece acolhimento, com modula√ß√£o de efeitos\",\n",
    "        \"Alguns pedidos prosperam, outros carecem de fundamenta√ß√£o adequada\",\n",
    "        \"Condena√ß√£o parcial procedente quanto aos danos materiais solicitados\",\n",
    "        \"Direito reconhecido em sua integralidade, exceto quanto √† indeniza√ß√£o por lucros cessantes\",\n",
    "        \n",
    "        # Conte√∫dos variados (neutros)\n",
    "        \"Conforme estatu√≠do no artigo 535 do C√≥digo de Processo Civil\",\n",
    "        \"A legisla√ß√£o processual estabelece requisitos formais espec√≠ficos\",\n",
    "        \"Compet√™ncia origin√°ria da Corte Superior observados os crit√©rios legais\",\n",
    "        \"Procedimento ordin√°rio com todas as fases processuais cumpridas adequadamente\",\n",
    "        \"Apela√ß√£o em conformidade com os prazos legalmente estabelecidos pelo c√≥digo processual\",\n",
    "    ]\n",
    "    \n",
    "    outcomes = ['Deferida', 'Indeferida', 'Parcialmente_Deferida']\n",
    "    \n",
    "    # Gerar dataset variado\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    for _ in range(DATASET_SIZE):\n",
    "        # Combinar templates para criar varia√ß√£o\n",
    "        num_templates = np.random.randint(2, 5)\n",
    "        selected = np.random.choice(corpus_templates, num_templates, replace=True)\n",
    "        text = \". \".join(selected) + \".\"\n",
    "        \n",
    "        # Atribuir label (com distribui√ß√£o realista)\n",
    "        label = np.random.choice(outcomes, p=[0.40, 0.35, 0.25])\n",
    "        \n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'texto_peticao': texts,\n",
    "        'desfecho': labels,\n",
    "        'data': pd.date_range('2015-01-01', periods=DATASET_SIZE, freq='h'),\n",
    "        'valor': np.random.uniform(1000, 500000, DATASET_SIZE)\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úì Dataset sint√©tico gerado: {df.shape}\")\n",
    "\n",
    "# An√°lise explorat√≥ria\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AN√ÅLISE EXPLORAT√ìRIA DO DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nPrimeiras amostras:\")\n",
    "print(df.head(10))\n",
    "\n",
    "print(f\"\\nDistribui√ß√£o de desfechos:\")\n",
    "distribution = df['desfecho'].value_counts()\n",
    "print(distribution)\n",
    "print(f\"\\nPercentual:\")\n",
    "print((df['desfecho'].value_counts(normalize=True) * 100).round(2))\n",
    "\n",
    "# Visualizar distribui√ß√£o\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "distribution.plot(kind='bar', edgecolor='black', alpha=0.7, ax=ax)\n",
    "ax.set_title('Distribui√ß√£o de Desfechos - Dataset Grande Escala', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Desfecho')\n",
    "ax.set_ylabel('Frequ√™ncia')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf89313",
   "metadata": {},
   "source": [
    "## 3. Pr√©-processamento de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18c913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Pr√©-processar texto para CNN\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    tokens = word_tokenize(text, language='portuguese')\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    stemmer = SnowballStemmer('portuguese')\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Pr√©-processar textos\n",
    "print(\"Pr√©-processando textos...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df['texto_processado'] = df['texto_peticao'].apply(preprocess_text)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"‚úì Pr√©-processamento conclu√≠do em {elapsed:.2f}s\")\n",
    "print(f\"  - Taxa: {len(df)/elapsed:.0f} textos/s\")\n",
    "\n",
    "print(f\"\\nExemplo antes: {df['texto_peticao'].iloc[0][:80]}...\")\n",
    "print(f\"Exemplo depois: {df['texto_processado'].iloc[0][:80]}...\")\n",
    "\n",
    "# Estat√≠sticas de comprimento\n",
    "text_lengths = df['texto_processado'].str.split().str.len()\n",
    "print(f\"\\nEstat√≠sticas de comprimento:\")\n",
    "print(f\"  - M√©dio: {text_lengths.mean():.0f} palavras\")\n",
    "print(f\"  - Min: {text_lengths.min()}, Max: {text_lengths.max()}\")\n",
    "print(f\"  - Mediana: {text_lengths.median():.0f}, Std: {text_lengths.std():.0f}\")\n",
    "\n",
    "# Visualizar distribui√ß√£o\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.hist(text_lengths, bins=50, edgecolor='black', alpha=0.7)\n",
    "ax.set_xlabel('N√∫mero de Palavras')\n",
    "ax.set_ylabel('Frequ√™ncia')\n",
    "ax.set_title('Distribui√ß√£o de Comprimento dos Textos Pr√©-processados')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057dda48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeniza√ß√£o com PyTorch\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOKENIZA√á√ÉO E PREPARA√á√ÉO DE SEQU√äNCIAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Construir vocabul√°rio\n",
    "print(\"\\nConstruindo vocabul√°rio...\")\n",
    "word_counts = Counter()\n",
    "for text in df['texto_processado']:\n",
    "    words = text.split()\n",
    "    word_counts.update(words)\n",
    "\n",
    "# Manter apenas as palavras mais frequentes\n",
    "vocabulary = {word: idx + 1 for idx, (word, count) in \n",
    "              enumerate(word_counts.most_common(VOCAB_SIZE - 1))}\n",
    "vocabulary['<UNK>'] = 0  # Token para palavras desconhecidas\n",
    "\n",
    "print(f\"‚úì Vocabul√°rio constru√≠do: {len(vocabulary)} palavras\")\n",
    "\n",
    "# Converter textos para sequ√™ncias de √≠ndices\n",
    "def text_to_sequence(text, vocab, max_len):\n",
    "    words = text.split()\n",
    "    sequence = [vocab.get(word, 0) for word in words]\n",
    "    \n",
    "    # Padding ou truncamento\n",
    "    if len(sequence) < max_len:\n",
    "        sequence += [0] * (max_len - len(sequence))\n",
    "    else:\n",
    "        sequence = sequence[:max_len]\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "print(\"\\nConvertendo textos para sequ√™ncias...\")\n",
    "X = np.array([text_to_sequence(text, vocabulary, MAX_SEQUENCE_LENGTH) \n",
    "              for text in df['texto_processado']])\n",
    "\n",
    "print(f\"‚úì Sequ√™ncias criadas: {X.shape}\")\n",
    "\n",
    "# Codificar labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['desfecho'])\n",
    "\n",
    "print(f\"‚úì Labels codificados: {label_encoder.classes_}\")\n",
    "print(f\"  - Mapeamento: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "\n",
    "# Dividir dados: 70% treino, 15% valida√ß√£o, 15% teste\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.15/(0.85), random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nDivis√£o do dataset:\")\n",
    "print(f\"  - Treino: {X_train.shape[0]:,} amostras ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Valida√ß√£o: {X_val.shape[0]:,} amostras ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Teste: {X_test.shape[0]:,} amostras ({X_test.shape[0]/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8101982e",
   "metadata": {},
   "source": [
    "## 4. Dataset e DataLoader em PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75672a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeticionDataset(Dataset):\n",
    "    \"\"\"Dataset customizado para peti√ß√µes jur√≠dicas\"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.LongTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Criar datasets\n",
    "train_dataset = PeticionDataset(X_train, y_train)\n",
    "val_dataset = PeticionDataset(X_val, y_val)\n",
    "test_dataset = PeticionDataset(X_test, y_test)\n",
    "\n",
    "# Hyperpar√¢metros\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Criar DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATALOADERS CRIADOS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Train DataLoader: {len(train_loader)} batches\")\n",
    "print(f\"‚úì Val DataLoader: {len(val_loader)} batches\")\n",
    "print(f\"‚úì Test DataLoader: {len(test_loader)} batches\")\n",
    "\n",
    "# Verificar um batch\n",
    "X_sample, y_sample = next(iter(train_loader))\n",
    "print(f\"\\nSample batch shape:\")\n",
    "print(f\"  - X: {X_sample.shape}\")\n",
    "print(f\"  - y: {y_sample.shape}\")\n",
    "print(f\"‚úì DataLoaders prontos para treinamento!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858d55d5",
   "metadata": {},
   "source": [
    "## 5. Arquitetura CNN 1D Profunda em PyTorch\n",
    "\n",
    "### Caracter√≠sticas:\n",
    "- **Embedding**: Converte √≠ndices em vetores densos (100D)\n",
    "- **Blocos Convolucionais**: 4 blocos com skip connections\n",
    "- **Filtros variados**: Tamanhos 3, 4, 5, 7\n",
    "- **Regulariza√ß√£o**: BatchNorm, Dropout (0.3-0.5)\n",
    "- **Pooling**: Global Average Pooling\n",
    "- **Camadas Densas**: 512 ‚Üí 256 ‚Üí num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91280bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConvBlock(nn.Module):\n",
    "    \"\"\"Bloco convolucional com residual connection\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding):\n",
    "        super(ResidualConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels, out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # Skip connection (1x1 conv se dimens√µes forem diferentes)\n",
    "        self.skip = nn.Conv1d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = self.skip(x)\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x + residual\n",
    "        return x\n",
    "\n",
    "class DeepCNN1D(nn.Module):\n",
    "    \"\"\"CNN 1D Profunda para classifica√ß√£o de peti√ß√µes\"\"\"\n",
    "    def __init__(self, vocab_size, num_classes, embedding_dim=100, hidden_dim=256):\n",
    "        super(DeepCNN1D, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Blocos convolucionais com diferentes kernel sizes\n",
    "        filter_sizes = [3, 4, 5, 7]\n",
    "        num_filters = 100\n",
    "        \n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            ResidualConvBlock(embedding_dim, num_filters, kernel_size, padding=kernel_size//2)\n",
    "            for kernel_size in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        # Pooling e dropout\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        \n",
    "        # Camadas densas\n",
    "        total_conv_out = len(filter_sizes) * num_filters\n",
    "        \n",
    "        self.fc1 = nn.Linear(total_conv_out, hidden_dim)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim // 2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # (batch, seq_len) -> (batch, seq_len, embed_dim)\n",
    "        x = x.transpose(1, 2)  # (batch, embed_dim, seq_len) para Conv1d\n",
    "        \n",
    "        # M√∫ltiplos blocos convolucionais em paralelo\n",
    "        conv_outputs = []\n",
    "        for conv_block in self.conv_blocks:\n",
    "            conv_out = conv_block(x)  # (batch, num_filters, seq_len)\n",
    "            pool_out = self.pool(conv_out)  # (batch, num_filters, 1)\n",
    "            pool_out = pool_out.squeeze(-1)  # (batch, num_filters)\n",
    "            conv_outputs.append(pool_out)\n",
    "        \n",
    "        # Concatenar outputs\n",
    "        x = torch.cat(conv_outputs, dim=1)  # (batch, total_conv_out)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Camadas densas\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn_fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn_fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "# Criar modelo\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONSTRUINDO MODELO CNN 1D PROFUNDO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = DeepCNN1D(\n",
    "    vocab_size=VOCAB_SIZE + 1,\n",
    "    num_classes=num_classes,\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=256\n",
    ").to(device)\n",
    "\n",
    "# Resumo do modelo\n",
    "print(f\"\\n‚úì Modelo criado com sucesso!\")\n",
    "print(f\"\\nArquitetura:\")\n",
    "print(model)\n",
    "\n",
    "# Contar par√¢metros\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nPar√¢metros do modelo:\")\n",
    "print(f\"  - Total: {total_params:,}\")\n",
    "print(f\"  - Trein√°veis: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f543c26a",
   "metadata": {},
   "source": [
    "## 6. Compilar e Configurar Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631d54ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONFIGURANDO TREINAMENTO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Hiperpar√¢metros\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 30\n",
    "PATIENCE = 5\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# Loss e otimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    verbose=True,\n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "# Vari√°veis para Early Stopping\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Hist√≥rico de treinamento\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úì Configura√ß√µes:\")\n",
    "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Epochs: {EPOCHS}\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Optimizer: AdamW (weight_decay={WEIGHT_DECAY})\")\n",
    "print(f\"  - Loss: CrossEntropyLoss\")\n",
    "print(f\"  - Early Stopping Patience: {PATIENCE} √©pocas\")\n",
    "print(f\"  - Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45c37d2",
   "metadata": {},
   "source": [
    "## 7. Fun√ß√µes de Treinamento e Avalia√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11c9031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Treinar uma √©poca\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=\"Treinamento\")\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Estat√≠sticas\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': loss.item():.4f}, refresh=True)\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validar modelo\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Importar tqdm para barra de progresso\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"‚úì Fun√ß√µes de treinamento definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ddbb3",
   "metadata": {},
   "source": [
    "## 8. Treinar Modelo (8. Treinar Modelo (pode levar 30-60 minutos em CPU, 5-10 minutos em GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4b564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TREINANDO MODELO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_training = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"√âpoca {epoch + 1}/{EPOCHS}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Treinar\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validar\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Guardar hist√≥rico\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Imprimir m√©tricas\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        print(f\"‚úì Melhor modelo salvo! Val Loss: {val_loss:.4f}\")\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Sem melhora. Paci√™ncia: {patience_counter}/{PATIENCE}\")\n",
    "        \n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"\\n‚ö† Early stopping acionado na √©poca {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "elapsed_training = time.time() - start_training\n",
    "print(f\"\\n‚úì Treinamento conclu√≠do em {elapsed_training/60:.2f} minutos\")\n",
    "\n",
    "# Carregar melhor modelo\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "print(\"‚úì Melhor modelo carregado para avalia√ß√£o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d6ce0",
   "metadata": {},
   "source": [
    "## 9. Avaliar no Conjunto de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0553b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AVALIA√á√ÉO NO CONJUNTO DE TESTE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Fazer predi√ß√µes no conjunto de teste\n",
    "model.eval()\n",
    "y_pred_list = []\n",
    "y_prob_list = []\n",
    "y_true_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        y_pred_list.extend(predicted.cpu().numpy())\n",
    "        y_prob_list.extend(probs.cpu().numpy())\n",
    "        y_true_list.extend(labels.numpy())\n",
    "\n",
    "y_pred = np.array(y_pred_list)\n",
    "y_prob = np.array(y_prob_list)\n",
    "y_true = np.array(y_true_list)\n",
    "\n",
    "# M√©tricas\n",
    "test_accuracy = accuracy_score(y_true, y_pred)\n",
    "test_precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "test_recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "test_f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\nM√©tricas no Conjunto de Teste:\")\n",
    "print(f\"  - Acur√°cia: {test_accuracy:.4f}\")\n",
    "print(f\"  - Precis√£o (ponderada): {test_precision:.4f}\")\n",
    "print(f\"  - Recall (ponderado): {test_recall:.4f}\")\n",
    "print(f\"  - F1-Score (ponderado): {test_f1:.4f}\")\n",
    "\n",
    "# Relat√≥rio de classifica√ß√£o\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RELAT√ìRIO DE CLASSIFICA√á√ÉO DETALHADO\")\n",
    "print(f\"{'='*80}\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_, digits=4))\n",
    "\n",
    "# Matriz de confus√£o\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(f\"\\nMatriz de Confus√£o:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e003bd",
   "metadata": {},
   "source": [
    "## 10. Realizar Predi√ß√µes em Novos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82978c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_petition(text, model, vocabulary, label_encoder, device, max_len=MAX_SEQUENCE_LENGTH):\n",
    "    \"\"\"Fazer predi√ß√£o para um novo texto\"\"\"\n",
    "    # Pr√©-processar\n",
    "    processed = preprocess_text(text)\n",
    "    \n",
    "    # Converter para sequ√™ncia\n",
    "    sequence = text_to_sequence(processed, vocabulary, max_len)\n",
    "    \n",
    "    # Converter para tensor\n",
    "    X = torch.LongTensor([sequence]).to(device)\n",
    "    \n",
    "    # Fazer predi√ß√£o\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(X)\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        predicted_class = torch.argmax(probs, dim=1).item()\n",
    "        confidence = probs[0, predicted_class].item()\n",
    "    \n",
    "    # Inverter labels\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_class])[0]\n",
    "    \n",
    "    return {\n",
    "        'texto': text[:100] + '...' if len(text) > 100 else text,\n",
    "        'desfecho_predito': predicted_label,\n",
    "        'confianca': confidence,\n",
    "        'probabilidades': {\n",
    "            label: float(probs[0, i].item()) \n",
    "            for i, label in enumerate(label_encoder.classes_)\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREDI√á√ïES EM NOVOS DADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Testar com exemplos do conjunto de teste\n",
    "print(\"\\nExemplos de predi√ß√µes do conjunto de teste:\\n\")\n",
    "\n",
    "indices = np.random.choice(len(X_test), min(5, len(X_test)), replace=False)\n",
    "for i, idx in enumerate(indices):\n",
    "    # Recuperar texto original\n",
    "    idx_original = np.where((X == X_test[idx]).all(axis=1))[0]\n",
    "    if len(idx_original) > 0:\n",
    "        texto_original = df['texto_peticao'].iloc[idx_original[0]]\n",
    "    else:\n",
    "        texto_original = \"Texto n√£o encontrado\"\n",
    "    \n",
    "    resultado_real = label_encoder.classes_[y_true[idx]]\n",
    "    resultado_predito = label_encoder.classes_[y_pred[idx]]\n",
    "    confianca = y_prob[idx, y_pred[idx]]\n",
    "    \n",
    "    print(f\"Exemplo {i+1}:\")\n",
    "    print(f\"  Texto: {texto_original[:80]}...\")\n",
    "    print(f\"  Resultado real: {resultado_real}\")\n",
    "    print(f\"  Resultado predito: {resultado_predito}\")\n",
    "    print(f\"  Confian√ßa: {confianca:.4f}\")\n",
    "    print(f\"  Status: {'‚úì Acerto' if resultado_real == resultado_predito else '‚úó Erro'}\")\n",
    "    print()\n",
    "\n",
    "# Testar com novos textos\n",
    "print(\"\\nPredi√ß√µes em novos textos de exemplo:\\n\")\n",
    "\n",
    "novos_textos = [\n",
    "    \"Recurso extraordin√°rio fundamentado em viola√ß√£o de direito constitucional com ampla jurisprud√™ncia consolidada\",\n",
    "    \"Peti√ß√£o inicial carente de elementos essenciais e faltando documenta√ß√£o b√°sica\",\n",
    "    \"Apela√ß√£o questionando parte da senten√ßa anterior com argumentos moderados\"\n",
    "]\n",
    "\n",
    "for texto in novos_textos:\n",
    "    resultado = predict_petition(texto, model, vocabulary, label_encoder, device)\n",
    "    print(f\"Texto: {resultado['texto']}\")\n",
    "    print(f\"Desfecho predito: {resultado['desfecho_predito']}\")\n",
    "    print(f\"Confian√ßa: {resultado['confianca']:.4f}\")\n",
    "    print(f\"Probabilidades por classe:\")\n",
    "    for classe, prob in resultado['probabilidades'].items():\n",
    "        print(f\"  - {classe}: {prob:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5487503",
   "metadata": {},
   "source": [
    "## 11. Visualizar Resultados e M√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f802595",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZA√á√ïES DE RESULTADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Hist√≥rico de Loss e Acur√°cia\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2, marker='o', markersize=4)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, marker='s', markersize=4)\n",
    "axes[0].set_xlabel('√âpoca', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Hist√≥rico de Loss durante Treinamento', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Acur√°cia\n",
    "axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2, marker='o', markersize=4)\n",
    "axes[1].plot(history['val_acc'], label='Val Accuracy', linewidth=2, marker='s', markersize=4)\n",
    "axes[1].set_xlabel('√âpoca', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Acur√°cia', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Hist√≥rico de Acur√°cia durante Treinamento', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Matriz de Confus√£o\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "plt.title('Matriz de Confus√£o - Conjunto de Teste', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. M√©tricas por classe\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "    y_true, y_pred, labels=range(len(label_encoder.classes_))\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(label_encoder.classes_))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, precision_per_class, width, label='Precis√£o', alpha=0.8)\n",
    "ax.bar(x, recall_per_class, width, label='Recall', alpha=0.8)\n",
    "ax.bar(x + width, f1_per_class, width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Classe', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('M√©tricas de Desempenho por Classe', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(label_encoder.classes_)\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim([0, 1.1])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Visualiza√ß√µes geradas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308b6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZA√á√ïES ADICIONAIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 4. Distribui√ß√£o de confian√ßa por classe\n",
    "fig, axes = plt.subplots(1, len(label_encoder.classes_), figsize=(16, 5))\n",
    "\n",
    "for idx, label in enumerate(label_encoder.classes_):\n",
    "    class_indices = y_true == idx\n",
    "    class_probs = y_prob[class_indices, idx]\n",
    "    \n",
    "    axes[idx].hist(class_probs, bins=30, edgecolor='black', alpha=0.7, color=f'C{idx}')\n",
    "    axes[idx].set_xlabel('Confian√ßa', fontsize=11)\n",
    "    axes[idx].set_ylabel('Frequ√™ncia', fontsize=11)\n",
    "    axes[idx].set_title(f'Distribui√ß√£o - {label}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlim([0, 1])\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Curvas ROC (One-vs-Rest)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_test_bin = label_binarize(y_true, classes=range(len(label_encoder.classes_)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, len(label_encoder.classes_)))\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, color=colors[i], lw=2.5, label=f'{label} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Classificador Aleat√≥rio')\n",
    "ax.set_xlabel('Taxa de Falso Positivo', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Taxa de Verdadeiro Positivo', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Curvas ROC - One-vs-Rest', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Distribui√ß√£o de predi√ß√µes vs realidade\n",
    "pred_dist = np.bincount(y_pred, minlength=len(label_encoder.classes_))\n",
    "true_dist = np.bincount(y_true, minlength=len(label_encoder.classes_))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(label_encoder.classes_))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, true_dist, width, label='Real', alpha=0.8)\n",
    "ax.bar(x + width/2, pred_dist, width, label='Predito', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Classe', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Frequ√™ncia', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Distribui√ß√£o Real vs Predita - Conjunto de Teste', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(label_encoder.classes_)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Todas as visualiza√ß√µes geradas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2be47c0",
   "metadata": {},
   "source": [
    "## 12. Salvar Modelo e Componentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6f7710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SALVANDO MODELO E COMPONENTES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Salvar modelo PyTorch\n",
    "torch.save(model.state_dict(), 'cnn_model_estado.pth')\n",
    "print(\"‚úì Estado do modelo salvo em 'cnn_model_estado.pth'\")\n",
    "\n",
    "# Salvar model completo (para carregamento direto)\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab_size': VOCAB_SIZE,\n",
    "    'num_classes': num_classes,\n",
    "    'embedding_dim': 100,\n",
    "    'hidden_dim': 256\n",
    "}, 'cnn_model_completo.pth')\n",
    "print(\"‚úì Modelo completo salvo em 'cnn_model_completo.pth'\")\n",
    "\n",
    "# Salvar vocabul√°rio\n",
    "with open('vocabulario.pkl', 'wb') as f:\n",
    "    pickle.dump(vocabulary, f)\n",
    "print(\"‚úì Vocabul√°rio salvo em 'vocabulario.pkl'\")\n",
    "\n",
    "# Salvar label encoder\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "print(\"‚úì Label Encoder salvo em 'label_encoder.pkl'\")\n",
    "\n",
    "# Salvar configura√ß√µes\n",
    "config = {\n",
    "    'vocab_size': VOCAB_SIZE,\n",
    "    'max_sequence_length': MAX_SEQUENCE_LENGTH,\n",
    "    'num_classes': num_classes,\n",
    "    'embedding_dim': 100,\n",
    "    'hidden_dim': 256,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'epochs_trained': len(history['train_loss']),\n",
    "    'classes': label_encoder.classes_.tolist()\n",
    "}\n",
    "\n",
    "with open('config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(\"‚úì Configura√ß√µes salvas em 'config.json'\")\n",
    "\n",
    "# Salvar hist√≥rico de treinamento\n",
    "history_df = pd.DataFrame(history)\n",
    "history_df.to_csv('historico_treinamento.csv', index=False)\n",
    "print(\"‚úì Hist√≥rico de treinamento salvo em 'historico_treinamento.csv'\")\n",
    "\n",
    "# Salvar m√©tricas finais\n",
    "metrics_final = {\n",
    "    'test_accuracy': float(test_accuracy),\n",
    "    'test_precision': float(test_precision),\n",
    "    'test_recall': float(test_recall),\n",
    "    'test_f1': float(test_f1),\n",
    "    'total_parameters': int(trainable_params),\n",
    "    'training_time_seconds': elapsed_training\n",
    "}\n",
    "\n",
    "with open('metricas_finais.json', 'w') as f:\n",
    "    json.dump(metrics_final, f, indent=2)\n",
    "print(\"‚úì M√©tricas finais salvas em 'metricas_finais.json'\")\n",
    "\n",
    "print(\"\\n‚úì Todos os componentes salvos com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288b30e5",
   "metadata": {},
   "source": [
    "## 13. Resumo Executivo e Recomenda√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e004f6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMO EXECUTIVO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "üìä DATASET\n",
    "  ‚Ä¢ Tamanho total: {len(df):,} amostras\n",
    "  ‚Ä¢ Treino: {len(X_train):,} ({len(X_train)/len(df)*100:.1f}%)\n",
    "  ‚Ä¢ Valida√ß√£o: {len(X_val):,} ({len(X_val)/len(df)*100:.1f}%)\n",
    "  ‚Ä¢ Teste: {len(X_test):,} ({len(X_test)/len(df)*100:.1f}%)\n",
    "  ‚Ä¢ Classes: {', '.join(label_encoder.classes_)}\n",
    "\n",
    "üèóÔ∏è ARQUITETURA\n",
    "  ‚Ä¢ Tipo: CNN 1D Profunda com Residual Connections\n",
    "  ‚Ä¢ Camadas Convolucionais: 4 blocos (kernels 3, 4, 5, 7)\n",
    "  ‚Ä¢ Par√¢metros totais: {total_params:,}\n",
    "  ‚Ä¢ Par√¢metros trein√°veis: {trainable_params:,}\n",
    "\n",
    "üìà TREINAMENTO\n",
    "  ‚Ä¢ √âpocas executadas: {len(history['train_loss'])}\n",
    "  ‚Ä¢ Tempo total: {elapsed_training/60:.2f} minutos\n",
    "  ‚Ä¢ Otimizador: AdamW (lr={LEARNING_RATE})\n",
    "  ‚Ä¢ Early Stopping: Paci√™ncia = {PATIENCE}\n",
    "  ‚Ä¢ Melhor val_loss: {best_val_loss:.4f}\n",
    "\n",
    "‚úÖ PERFORMANCE NO TESTE\n",
    "  ‚Ä¢ Acur√°cia: {test_accuracy:.4f}\n",
    "  ‚Ä¢ Precis√£o (ponderada): {test_precision:.4f}\n",
    "  ‚Ä¢ Recall (ponderado): {test_recall:.4f}\n",
    "  ‚Ä¢ F1-Score (ponderado): {test_f1:.4f}\n",
    "\n",
    "üìå RECOMENDA√á√ïES PARA MELHORIAS\n",
    "  1. Usar word embeddings pr√©-treinados (Word2Vec, GloVe, FastText)\n",
    "  2. Implementar attention mechanisms para capturar contexto\n",
    "  3. Treinar com mais amostras (se dispon√≠vel)\n",
    "  4. Usar Transformer-based models (BERT, DistilBERT)\n",
    "  5. Aplicar data augmentation (paraphrasing, back-translation)\n",
    "  6. Realizar an√°lise de features mais importantes\n",
    "  7. Implementar ensemble methods (vota√ß√£o de m√∫ltiplos modelos)\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚úì NOTEBOOK CONCLU√çDO COM SUCESSO!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
