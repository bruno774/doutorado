{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "776dfc01",
   "metadata": {},
   "source": [
    "# Disciplina Aprendizado Profundo\n",
    "\n",
    "### Prof. Josenalde Barbosa - IMD/UFRN\n",
    "\n",
    "### Aluno: Bruno Santos F. Silva (matr. 2025)\n",
    "\n",
    "#### Objetivo: elaborar um classificador usando redes neurais profundas LSTM e pytorch com a finalidade de identificar processos judiciais que ser√£o deferidos, indeferidos ou deferidos parcialmente.\n",
    "\n",
    "Obs. Atividade avaliativa final da disciplina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a1f281",
   "metadata": {},
   "source": [
    "## 1. Importa√ß√£o de Bibliotecas Essenciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b1508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import warnings\n",
    "import time\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# Processamento de texto\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# M√©tricas e avalia√ß√£o\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configura√ß√µes\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Download NLTK resources\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Configurar device (GPU se dispon√≠vel)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úì Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úì CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "print(f\"‚úì PyTorch vers√£o: {torch.__version__}\")\n",
    "print(f\"‚úì NumPy vers√£o: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed7338d",
   "metadata": {},
   "source": [
    "## 2. Carregar e Gerar Dataset de Grande Escala (80k amostras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae53a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par√¢metros\n",
    "DB_PATH = \"/gdrive/MyDrive/Colab Notebooks/JFRN/split_dados_01.sqlite3\"\n",
    "TABLE_NAME = \"peticoes\"\n",
    "DATASET_SIZE = 80000\n",
    "\n",
    "print(f\"Carregando/Gerando dataset com {DATASET_SIZE:,} amostras...\")\n",
    "\n",
    "# Tentar carregar do SQLite\n",
    "try:\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    query = f\"SELECT * FROM {TABLE_NAME} LIMIT {DATASET_SIZE}\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    print(f\"‚úì Dados carregados do SQLite: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Erro ao carregar SQLite: {e}\")\n",
    "    print(f\"Gerando dataset sint√©tico com {DATASET_SIZE:,} amostras...\\n\")\n",
    "    \n",
    "    # Corpus variado de peti√ß√µes jur√≠dicas\n",
    "    corpus_templates = [\n",
    "        # Peti√ß√µes deferidas (linguagem favor√°vel)\n",
    "        \"Fundamentado em jurisprud√™ncia consolidada, solicita-se deferimento integral do pedido\",\n",
    "        \"Documenta√ß√£o completa e direito inequ√≠voco justificam a concess√£o pleiteada\",\n",
    "        \"Precedentes do STJ e STF amplaram o reconhecimento desta modalidade de direito\",\n",
    "        \"Ampla documenta√ß√£o apresentada comprova inequivocamente o direito invocado\",\n",
    "        \"Recurso extraordin√°rio com repercuss√£o geral reconhecida sobre mat√©ria j√° pacificada\",\n",
    "        \n",
    "        # Peti√ß√µes indeferidas (linguagem desfavor√°vel)\n",
    "        \"Faltam elementos essenciais √† constitui√ß√£o da rela√ß√£o jur√≠dica alegada\",\n",
    "        \"Prescri√ß√£o consumada extingue o direito de a√ß√£o pelo lapso temporal\",\n",
    "        \"Lacks documentary evidence and legal grounds for the claim presented\",\n",
    "        \"Recurso manifestamente infundado contradiz jurisprud√™ncia consolidada\",\n",
    "        \"Car√™ncia de legitimidade ativa e passiva invalida a demanda\",\n",
    "        \n",
    "        # Peti√ß√µes parcialmente deferidas (linguagem mista)\n",
    "        \"Parcialmente fundado o recurso, reconhecendo-se apenas parte da pretens√£o\",\n",
    "        \"Parte do pleito merece acolhimento, com modula√ß√£o de efeitos\",\n",
    "        \"Alguns pedidos prosperam, outros carecem de fundamenta√ß√£o adequada\",\n",
    "        \"Condena√ß√£o parcial procedente quanto aos danos materiais solicitados\",\n",
    "        \"Direito reconhecido em sua integralidade, exceto quanto √† indeniza√ß√£o por lucros cessantes\",\n",
    "        \n",
    "        # Conte√∫dos variados (neutros)\n",
    "        \"Conforme estatu√≠do no artigo 535 do C√≥digo de Processo Civil\",\n",
    "        \"A legisla√ß√£o processual estabelece requisitos formais espec√≠ficos\",\n",
    "        \"Compet√™ncia origin√°ria da Corte Superior observados os crit√©rios legais\",\n",
    "        \"Procedimento ordin√°rio com todas as fases processuais cumpridas adequadamente\",\n",
    "        \"Apela√ß√£o em conformidade com os prazos legalmente estabelecidos pelo c√≥digo processual\",\n",
    "    ]\n",
    "    \n",
    "    outcomes = ['Deferida', 'Indeferida', 'Parcialmente_Deferida']\n",
    "    \n",
    "    # Gerar dataset variado\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    for _ in range(DATASET_SIZE):\n",
    "        # Combinar templates para criar varia√ß√£o\n",
    "        num_templates = np.random.randint(2, 5)\n",
    "        selected = np.random.choice(corpus_templates, num_templates, replace=True)\n",
    "        text = \". \".join(selected) + \".\"\n",
    "        \n",
    "        # Atribuir label (com distribui√ß√£o realista)\n",
    "        label = np.random.choice(outcomes, p=[0.40, 0.35, 0.25])\n",
    "        \n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'texto_peticao': texts,\n",
    "        'desfecho': labels,\n",
    "        'data': pd.date_range('2015-01-01', periods=DATASET_SIZE, freq='h'),\n",
    "        'valor': np.random.uniform(1000, 500000, DATASET_SIZE)\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úì Dataset sint√©tico gerado: {df.shape}\")\n",
    "\n",
    "# An√°lise explorat√≥ria\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AN√ÅLISE EXPLORAT√ìRIA DO DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nPrimeiras amostras:\")\n",
    "print(df.head(10))\n",
    "\n",
    "print(f\"\\nDistribui√ß√£o de desfechos:\")\n",
    "distribution = df['desfecho'].value_counts()\n",
    "print(distribution)\n",
    "print(f\"\\nPercentual:\")\n",
    "print((df['desfecho'].value_counts(normalize=True) * 100).round(2))\n",
    "\n",
    "# Visualizar distribui√ß√£o\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "distribution.plot(kind='bar', edgecolor='black', alpha=0.7, ax=ax)\n",
    "ax.set_title('Distribui√ß√£o de Desfechos - Dataset Grande Escala', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Desfecho')\n",
    "ax.set_ylabel('Frequ√™ncia')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757f7030",
   "metadata": {},
   "source": [
    "## 3. Pr√©-processamento de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194eee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Pr√©-processar texto para LSTM\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    tokens = word_tokenize(text, language='portuguese')\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    stemmer = SnowballStemmer('portuguese')\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Pr√©-processar textos\n",
    "print(\"Pr√©-processando textos...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df['texto_processado'] = df['texto_peticao'].apply(preprocess_text)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"‚úì Pr√©-processamento conclu√≠do em {elapsed:.2f}s\")\n",
    "print(f\"  - Taxa: {len(df)/elapsed:.0f} textos/s\")\n",
    "\n",
    "print(f\"\\nExemplo antes: {df['texto_peticao'].iloc[0][:80]}...\")\n",
    "print(f\"Exemplo depois: {df['texto_processado'].iloc[0][:80]}...\")\n",
    "\n",
    "# Estat√≠sticas de comprimento\n",
    "text_lengths = df['texto_processado'].str.split().str.len()\n",
    "print(f\"\\nEstat√≠sticas de comprimento:\")\n",
    "print(f\"  - M√©dio: {text_lengths.mean():.0f} palavras\")\n",
    "print(f\"  - Min: {text_lengths.min()}, Max: {text_lengths.max()}\")\n",
    "print(f\"  - Mediana: {text_lengths.median():.0f}, Std: {text_lengths.std():.0f}\")\n",
    "\n",
    "# Visualizar distribui√ß√£o\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.hist(text_lengths, bins=50, edgecolor='black', alpha=0.7)\n",
    "ax.set_xlabel('N√∫mero de Palavras')\n",
    "ax.set_ylabel('Frequ√™ncia')\n",
    "ax.set_title('Distribui√ß√£o de Comprimento dos Textos Pr√©-processados')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cdb57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeniza√ß√£o com PyTorch\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOKENIZA√á√ÉO E PREPARA√á√ÉO DE SEQU√äNCIAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Construir vocabul√°rio\n",
    "print(\"\\nConstruindo vocabul√°rio...\")\n",
    "word_counts = Counter()\n",
    "for text in df['texto_processado']:\n",
    "    words = text.split()\n",
    "    word_counts.update(words)\n",
    "\n",
    "# Manter apenas as palavras mais frequentes\n",
    "vocabulary = {word: idx + 1 for idx, (word, count) in \n",
    "              enumerate(word_counts.most_common(VOCAB_SIZE - 1))}\n",
    "vocabulary['<UNK>'] = 0  # Token para palavras desconhecidas\n",
    "\n",
    "print(f\"‚úì Vocabul√°rio constru√≠do: {len(vocabulary)} palavras\")\n",
    "\n",
    "# Converter textos para sequ√™ncias de √≠ndices\n",
    "def text_to_sequence(text, vocab, max_len):\n",
    "    words = text.split()\n",
    "    sequence = [vocab.get(word, 0) for word in words]\n",
    "    \n",
    "    # Padding ou truncamento\n",
    "    if len(sequence) < max_len:\n",
    "        sequence += [0] * (max_len - len(sequence))\n",
    "    else:\n",
    "        sequence = sequence[:max_len]\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "print(\"\\nConvertendo textos para sequ√™ncias...\")\n",
    "X = np.array([text_to_sequence(text, vocabulary, MAX_SEQUENCE_LENGTH) \n",
    "              for text in df['texto_processado']])\n",
    "\n",
    "print(f\"‚úì Sequ√™ncias criadas: {X.shape}\")\n",
    "\n",
    "# Codificar labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['desfecho'])\n",
    "\n",
    "print(f\"‚úì Labels codificados: {label_encoder.classes_}\")\n",
    "print(f\"  - Mapeamento: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "\n",
    "# Dividir dados: 70% treino, 15% valida√ß√£o, 15% teste\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.15/(0.85), random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nDivis√£o do dataset:\")\n",
    "print(f\"  - Treino: {X_train.shape[0]:,} amostras ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Valida√ß√£o: {X_val.shape[0]:,} amostras ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Teste: {X_test.shape[0]:,} amostras ({X_test.shape[0]/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140642c0",
   "metadata": {},
   "source": [
    "## 4. Dataset e DataLoader em PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4582aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeticionDataset(Dataset):\n",
    "    \"\"\"Dataset customizado para peti√ß√µes jur√≠dicas\"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.LongTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Criar datasets\n",
    "train_dataset = PeticionDataset(X_train, y_train)\n",
    "val_dataset = PeticionDataset(X_val, y_val)\n",
    "test_dataset = PeticionDataset(X_test, y_test)\n",
    "\n",
    "# Hyperpar√¢metros\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Criar DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATALOADERS CRIADOS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Train DataLoader: {len(train_loader)} batches\")\n",
    "print(f\"‚úì Val DataLoader: {len(val_loader)} batches\")\n",
    "print(f\"‚úì Test DataLoader: {len(test_loader)} batches\")\n",
    "\n",
    "# Verificar um batch\n",
    "X_sample, y_sample = next(iter(train_loader))\n",
    "print(f\"\\nSample batch shape:\")\n",
    "print(f\"  - X: {X_sample.shape}\")\n",
    "print(f\"  - y: {y_sample.shape}\")\n",
    "print(f\"‚úì DataLoaders prontos para treinamento!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9681199",
   "metadata": {},
   "source": [
    "## 5. Arquitetura LSTM Bidirecional em PyTorch\n",
    "\n",
    "### Caracter√≠sticas:\n",
    "- **Embedding**: Converte √≠ndices em vetores densos (150D)\n",
    "- **LSTM Bidirecional**: 2 camadas, 256 unidades por dire√ß√£o\n",
    "- **Attention Mechanism**: Para focar em partes relevantes do texto\n",
    "- **Regulariza√ß√£o**: Dropout (0.4-0.5), LayerNorm\n",
    "- **Camadas Densas**: 256 ‚Üí 128 ‚Üí num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c0f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Mecanismo de aten√ß√£o para LSTM\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)  # Bidirecional\n",
    "    \n",
    "    def forward(self, lstm_output):\n",
    "        # lstm_output: (batch, seq_len, hidden_dim * 2)\n",
    "        attention_weights = torch.tanh(self.attention(lstm_output))  # (batch, seq_len, 1)\n",
    "        attention_weights = F.softmax(attention_weights, dim=1)  # (batch, seq_len, 1)\n",
    "        \n",
    "        # Aplicar pesos de aten√ß√£o\n",
    "        attended_output = torch.sum(attention_weights * lstm_output, dim=1)  # (batch, hidden_dim * 2)\n",
    "        return attended_output, attention_weights\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    \"\"\"LSTM Bidirecional para classifica√ß√£o de peti√ß√µes\"\"\"\n",
    "    def __init__(self, vocab_size, num_classes, embedding_dim=150, hidden_dim=256, num_layers=2):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # LSTM bidirecional\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=0.3 if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
    "        \n",
    "        # Camadas densas\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim // 2)\n",
    "        \n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.fc_out = nn.Linear(hidden_dim // 2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # (batch, seq_len) -> (batch, seq_len, embed_dim)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)  # lstm_out: (batch, seq_len, hidden_dim * 2)\n",
    "        \n",
    "        # Attention\n",
    "        attended_output, attention_weights = self.attention(lstm_out)  # (batch, hidden_dim * 2)\n",
    "        \n",
    "        # Layer normalization\n",
    "        attended_output = self.layer_norm(attended_output)\n",
    "        \n",
    "        # Camadas densas\n",
    "        x = self.dropout1(attended_output)\n",
    "        x = self.fc1(x)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Criar modelo\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONSTRUINDO MODELO LSTM BIDIRECIONAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = BidirectionalLSTM(\n",
    "    vocab_size=VOCAB_SIZE + 1,\n",
    "    num_classes=num_classes,\n",
    "    embedding_dim=150,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2\n",
    ").to(device)\n",
    "\n",
    "# Resumo do modelo\n",
    "print(f\"\\n‚úì Modelo criado com sucesso!\")\n",
    "print(f\"\\nArquitetura:\")\n",
    "print(model)\n",
    "\n",
    "# Contar par√¢metros\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nPar√¢metros do modelo:\")\n",
    "print(f\"  - Total: {total_params:,}\")\n",
    "print(f\"  - Trein√°veis: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f6e70b",
   "metadata": {},
   "source": [
    "## 6. Compilar e Configurar Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a8650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONFIGURANDO TREINAMENTO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Hiperpar√¢metros\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 30\n",
    "PATIENCE = 5\n",
    "WEIGHT_DECAY = 1e-5\n",
    "GRADIENT_CLIP = 1.0\n",
    "\n",
    "# Loss e otimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    verbose=True,\n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "# Vari√°veis para Early Stopping\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Hist√≥rico de treinamento\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úì Configura√ß√µes:\")\n",
    "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Epochs: {EPOCHS}\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Optimizer: AdamW (weight_decay={WEIGHT_DECAY})\")\n",
    "print(f\"  - Loss: CrossEntropyLoss\")\n",
    "print(f\"  - Gradient Clipping: {GRADIENT_CLIP}\")\n",
    "print(f\"  - Early Stopping Patience: {PATIENCE} √©pocas\")\n",
    "print(f\"  - Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b4ace0",
   "metadata": {},
   "source": [
    "## 7. Fun√ß√µes de Treinamento e Avalia√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc09dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device, gradient_clip):\n",
    "    \"\"\"Treinar uma √©poca\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=\"Treinamento\")\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Estat√≠sticas\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': loss.item():.4f}, refresh=True)\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validar modelo\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Importar tqdm para barra de progresso\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"‚úì Fun√ß√µes de treinamento definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ad3fd1",
   "metadata": {},
   "source": [
    "## 8. Treinar Modelo (pode levar 45-90 minutos em CPU, 10-20 minutos em GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51c1ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TREINANDO MODELO LSTM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_training = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"√âpoca {epoch + 1}/{EPOCHS}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Treinar\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, GRADIENT_CLIP)\n",
    "    \n",
    "    # Validar\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Guardar hist√≥rico\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Imprimir m√©tricas\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        print(f\"‚úì Melhor modelo salvo! Val Loss: {val_loss:.4f}\")\n",
    "        torch.save(model.state_dict(), 'best_lstm_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Sem melhora. Paci√™ncia: {patience_counter}/{PATIENCE}\")\n",
    "        \n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"\\n‚ö† Early stopping acionado na √©poca {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "elapsed_training = time.time() - start_training\n",
    "print(f\"\\n‚úì Treinamento conclu√≠do em {elapsed_training/60:.2f} minutos\")\n",
    "\n",
    "# Carregar melhor modelo\n",
    "model.load_state_dict(torch.load('best_lstm_model.pth'))\n",
    "print(\"‚úì Melhor modelo carregado para avalia√ß√£o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c66229",
   "metadata": {},
   "source": [
    "## 9. Avaliar no Conjunto de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc4eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AVALIA√á√ÉO NO CONJUNTO DE TESTE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Fazer predi√ß√µes no conjunto de teste\n",
    "model.eval()\n",
    "y_pred_list = []\n",
    "y_prob_list = []\n",
    "y_true_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        y_pred_list.extend(predicted.cpu().numpy())\n",
    "        y_prob_list.extend(probs.cpu().numpy())\n",
    "        y_true_list.extend(labels.numpy())\n",
    "\n",
    "y_pred = np.array(y_pred_list)\n",
    "y_prob = np.array(y_prob_list)\n",
    "y_true = np.array(y_true_list)\n",
    "\n",
    "# M√©tricas\n",
    "test_accuracy = accuracy_score(y_true, y_pred)\n",
    "test_precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "test_recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "test_f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\nM√©tricas no Conjunto de Teste:\")\n",
    "print(f\"  - Acur√°cia: {test_accuracy:.4f}\")\n",
    "print(f\"  - Precis√£o (ponderada): {test_precision:.4f}\")\n",
    "print(f\"  - Recall (ponderado): {test_recall:.4f}\")\n",
    "print(f\"  - F1-Score (ponderado): {test_f1:.4f}\")\n",
    "\n",
    "# Relat√≥rio de classifica√ß√£o\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RELAT√ìRIO DE CLASSIFICA√á√ÉO DETALHADO\")\n",
    "print(f\"{'='*80}\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_, digits=4))\n",
    "\n",
    "# Matriz de confus√£o\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(f\"\\nMatriz de Confus√£o:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5360187",
   "metadata": {},
   "source": [
    "## 10. Realizar Predi√ß√µes em Novos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2146e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_petition(text, model, vocabulary, label_encoder, device, max_len=MAX_SEQUENCE_LENGTH):\n",
    "    \"\"\"Fazer predi√ß√£o para um novo texto\"\"\"\n",
    "    # Pr√©-processar\n",
    "    processed = preprocess_text(text)\n",
    "    \n",
    "    # Converter para sequ√™ncia\n",
    "    sequence = text_to_sequence(processed, vocabulary, max_len)\n",
    "    \n",
    "    # Converter para tensor\n",
    "    X = torch.LongTensor([sequence]).to(device)\n",
    "    \n",
    "    # Fazer predi√ß√£o\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(X)\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        predicted_class = torch.argmax(probs, dim=1).item()\n",
    "        confidence = probs[0, predicted_class].item()\n",
    "    \n",
    "    # Inverter labels\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_class])[0]\n",
    "    \n",
    "    return {\n",
    "        'texto': text[:100] + '...' if len(text) > 100 else text,\n",
    "        'desfecho_predito': predicted_label,\n",
    "        'confianca': confidence,\n",
    "        'probabilidades': {\n",
    "            label: float(probs[0, i].item()) \n",
    "            for i, label in enumerate(label_encoder.classes_)\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREDI√á√ïES EM NOVOS DADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Testar com exemplos do conjunto de teste\n",
    "print(\"\\nExemplos de predi√ß√µes do conjunto de teste:\\n\")\n",
    "\n",
    "indices = np.random.choice(len(X_test), min(5, len(X_test)), replace=False)\n",
    "for i, idx in enumerate(indices):\n",
    "    # Recuperar texto original\n",
    "    idx_original = np.where((X == X_test[idx]).all(axis=1))[0]\n",
    "    if len(idx_original) > 0:\n",
    "        texto_original = df['texto_peticao'].iloc[idx_original[0]]\n",
    "    else:\n",
    "        texto_original = \"Texto n√£o encontrado\"\n",
    "    \n",
    "    resultado_real = label_encoder.classes_[y_true[idx]]\n",
    "    resultado_predito = label_encoder.classes_[y_pred[idx]]\n",
    "    confianca = y_prob[idx, y_pred[idx]]\n",
    "    \n",
    "    print(f\"Exemplo {i+1}:\")\n",
    "    print(f\"  Texto: {texto_original[:80]}...\")\n",
    "    print(f\"  Resultado real: {resultado_real}\")\n",
    "    print(f\"  Resultado predito: {resultado_predito}\")\n",
    "    print(f\"  Confian√ßa: {confianca:.4f}\")\n",
    "    print(f\"  Status: {'‚úì Acerto' if resultado_real == resultado_predito else '‚úó Erro'}\")\n",
    "    print()\n",
    "\n",
    "# Testar com novos textos\n",
    "print(\"\\nPredi√ß√µes em novos textos de exemplo:\\n\")\n",
    "\n",
    "novos_textos = [\n",
    "    \"Recurso extraordin√°rio fundamentado em viola√ß√£o de direito constitucional com ampla jurisprud√™ncia consolidada\",\n",
    "    \"Peti√ß√£o inicial carente de elementos essenciais e faltando documenta√ß√£o b√°sica\",\n",
    "    \"Apela√ß√£o questionando parte da senten√ßa anterior com argumentos moderados\"\n",
    "]\n",
    "\n",
    "for texto in novos_textos:\n",
    "    resultado = predict_petition(texto, model, vocabulary, label_encoder, device)\n",
    "    print(f\"Texto: {resultado['texto']}\")\n",
    "    print(f\"Desfecho predito: {resultado['desfecho_predito']}\")\n",
    "    print(f\"Confian√ßa: {resultado['confianca']:.4f}\")\n",
    "    print(f\"Probabilidades por classe:\")\n",
    "    for classe, prob in resultado['probabilidades'].items():\n",
    "        print(f\"  - {classe}: {prob:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea67b1",
   "metadata": {},
   "source": [
    "## 11. Visualiza√ß√µes e An√°lises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c01366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZA√á√ïES E AN√ÅLISES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Gr√°fico de hist√≥rico de treinamento\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history['train_loss'], label='Treino', marker='o')\n",
    "plt.plot(history['val_loss'], label='Valida√ß√£o', marker='o')\n",
    "plt.title('Evolu√ß√£o da Loss')\n",
    "plt.xlabel('√âpoca')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history['train_acc'], label='Treino', marker='o')\n",
    "plt.plot(history['val_acc'], label='Valida√ß√£o', marker='o')\n",
    "plt.title('Evolu√ß√£o da Acur√°cia')\n",
    "plt.xlabel('√âpoca')\n",
    "plt.ylabel('Acur√°cia')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# F1-Score\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history['train_f1'], label='Treino', marker='o')\n",
    "plt.plot(history['val_f1'], label='Valida√ß√£o', marker='o')\n",
    "plt.title('Evolu√ß√£o do F1-Score')\n",
    "plt.xlabel('√âpoca')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Matriz de Confus√£o\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title('Matriz de Confus√£o - LSTM Bidirecional com Aten√ß√£o')\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Real')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Relat√≥rio de Classifica√ß√£o Detalhado\n",
    "print(\"\\nRelat√≥rio de Classifica√ß√£o Detalhado:\")\n",
    "print(\"-\" * 60)\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# 4. Curvas ROC (para problemas multiclasse)\n",
    "if len(label_encoder.classes_) > 2:\n",
    "    # Binarizar labels para multiclasse\n",
    "    y_true_bin = label_binarize(y_true, classes=range(len(label_encoder.classes_)))\n",
    "    y_prob_bin = y_prob\n",
    "\n",
    "    # Calcular ROC para cada classe\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    for i in range(len(label_encoder.classes_)):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_prob_bin[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Plotar curvas ROC\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\n",
    "    \n",
    "    for i, color in zip(range(len(label_encoder.classes_)), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label=f'{label_encoder.classes_[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Taxa de Falsos Positivos')\n",
    "    plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "    plt.title('Curvas ROC - Multiclasse')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    # AUC m√©dia\n",
    "    auc_mean = np.mean(list(roc_auc.values()))\n",
    "    print(f\"\\nAUC M√©dio: {auc_mean:.4f}\")\n",
    "\n",
    "# 5. Distribui√ß√£o de probabilidades por classe\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, classe in enumerate(label_encoder.classes_):\n",
    "    plt.subplot(2, (len(label_encoder.classes_)+1)//2, i+1)\n",
    "    plt.hist(y_prob[:, i], bins=20, alpha=0.7, edgecolor='black')\n",
    "    plt.title(f'Distribui√ß√£o de Probabilidades\\n{classe}')\n",
    "    plt.xlabel('Probabilidade')\n",
    "    plt.ylabel('Frequ√™ncia')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. An√°lise de erros\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AN√ÅLISE DE ERROS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Encontrar exemplos classificados incorretamente\n",
    "erros_idx = np.where(y_true != y_pred)[0]\n",
    "print(f\"\\nTotal de erros: {len(erros_idx)} de {len(y_true)} ({len(erros_idx)/len(y_true)*100:.2f}%)\")\n",
    "\n",
    "if len(erros_idx) > 0:\n",
    "    print(\"\\nExemplos de erros de classifica√ß√£o:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Mostrar alguns exemplos de erros\n",
    "    for i in range(min(5, len(erros_idx))):\n",
    "        idx = erros_idx[i]\n",
    "        idx_original = np.where((X == X_test[idx]).all(axis=1))[0]\n",
    "        if len(idx_original) > 0:\n",
    "            texto_original = df['texto_peticao'].iloc[idx_original[0]]\n",
    "        else:\n",
    "            texto_original = \"Texto n√£o encontrado\"\n",
    "        \n",
    "        real = label_encoder.classes_[y_true[idx]]\n",
    "        predito = label_encoder.classes_[y_pred[idx]]\n",
    "        conf = y_prob[idx, y_pred[idx]]\n",
    "        \n",
    "        print(f\"Erro {i+1}:\")\n",
    "        print(f\"  Texto: {texto_original[:100]}...\")\n",
    "        print(f\"  Real: {real} | Predito: {predito} | Confian√ßa: {conf:.4f}\")\n",
    "        print()\n",
    "\n",
    "# 7. Estat√≠sticas por classe\n",
    "print(\"\\nEstat√≠sticas por Classe:\")\n",
    "print(\"-\" * 40)\n",
    "for i, classe in enumerate(label_encoder.classes_):\n",
    "    total_classe = np.sum(y_true == i)\n",
    "    acertos_classe = np.sum((y_true == i) & (y_pred == i))\n",
    "    precisao_classe = acertos_classe / total_classe if total_classe > 0 else 0\n",
    "    \n",
    "    print(f\"{classe}:\")\n",
    "    print(f\"  Total de exemplos: {total_classe}\")\n",
    "    print(f\"  Acertos: {acertos_classe}\")\n",
    "    print(f\"  Precis√£o: {precisao_classe:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22f53af",
   "metadata": {},
   "source": [
    "## 12. Salvamento do Modelo e Resumo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab393c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SALVAMENTO DO MODELO E RESUMO FINAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Criar diret√≥rio para salvar o modelo\n",
    "model_dir = 'modelos_lstm'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Timestamp para identifica√ß√£o √∫nica\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name = f\"lstm_bidirectional_attention_{timestamp}\"\n",
    "\n",
    "# Salvar o modelo PyTorch\n",
    "model_path = os.path.join(model_dir, f\"{model_name}.pth\")\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'epoch': len(history['train_loss']),\n",
    "    'history': history,\n",
    "    'config': {\n",
    "        'vocab_size': VOCAB_SIZE,\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'hidden_dim': HIDDEN_DIM,\n",
    "        'num_layers': NUM_LAYERS,\n",
    "        'num_classes': len(label_encoder.classes_),\n",
    "        'dropout': DROPOUT,\n",
    "        'max_sequence_length': MAX_SEQUENCE_LENGTH,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE\n",
    "    }\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Modelo salvo em: {model_path}\")\n",
    "\n",
    "# Salvar vocabul√°rio e label encoder\n",
    "vocab_path = os.path.join(model_dir, f\"{model_name}_vocab.json\")\n",
    "with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        'word_to_idx': vocabulary,\n",
    "        'idx_to_word': {v: k for k, v in vocabulary.items()},\n",
    "        'vocab_size': len(vocabulary)\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "label_encoder_path = os.path.join(model_dir, f\"{model_name}_labels.pkl\")\n",
    "import pickle\n",
    "with open(label_encoder_path, 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(f\"Vocabul√°rio salvo em: {vocab_path}\")\n",
    "print(f\"Label encoder salvo em: {label_encoder_path}\")\n",
    "\n",
    "# Salvar m√©tricas finais\n",
    "metrics_path = os.path.join(model_dir, f\"{model_name}_metrics.json\")\n",
    "final_metrics = {\n",
    "    'timestamp': timestamp,\n",
    "    'model_type': 'LSTM Bidirecional com Aten√ß√£o',\n",
    "    'dataset_size': len(df),\n",
    "    'train_size': len(X_train),\n",
    "    'val_size': len(X_val),\n",
    "    'test_size': len(X_test),\n",
    "    'vocabulary_size': len(vocabulary),\n",
    "    'max_sequence_length': MAX_SEQUENCE_LENGTH,\n",
    "    'best_epoch': best_epoch,\n",
    "    'training_time_minutes': training_time,\n",
    "    'final_train_loss': history['train_loss'][-1],\n",
    "    'final_val_loss': history['val_loss'][-1],\n",
    "    'final_train_acc': history['train_acc'][-1],\n",
    "    'final_val_acc': history['val_acc'][-1],\n",
    "    'final_train_f1': history['train_f1'][-1],\n",
    "    'final_val_f1': history['val_f1'][-1],\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'test_f1_macro': test_f1_macro,\n",
    "    'test_f1_weighted': test_f1_weighted,\n",
    "    'classes': list(label_encoder.classes_),\n",
    "    'architecture': {\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'hidden_dim': HIDDEN_DIM,\n",
    "        'num_layers': NUM_LAYERS,\n",
    "        'bidirectional': True,\n",
    "        'attention': True,\n",
    "        'dropout': DROPOUT\n",
    "    },\n",
    "    'training_config': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "        'patience': PATIENCE,\n",
    "        'max_epochs': MAX_EPOCHS,\n",
    "        'gradient_clip': GRADIENT_CLIP\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(metrics_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_metrics, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"M√©tricas salvas em: {metrics_path}\")\n",
    "\n",
    "# Resumo final\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMO FINAL - CLASSIFICADOR LSTM PARA PETI√á√ïES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "üìä DESEMPENHO GERAL:\n",
    "   ‚Ä¢ Acur√°cia no teste: {test_accuracy:.4f}\n",
    "   ‚Ä¢ F1-Score Macro: {test_f1_macro:.4f}\n",
    "   ‚Ä¢ F1-Score Ponderado: {test_f1_weighted:.4f}\n",
    "   ‚Ä¢ Melhor √©poca: {best_epoch}\n",
    "   ‚Ä¢ Tempo de treinamento: {training_time:.1f} minutos\n",
    "\n",
    "üèóÔ∏è ARQUITETURA:\n",
    "   ‚Ä¢ Modelo: LSTM Bidirecional com Mecanismo de Aten√ß√£o\n",
    "   ‚Ä¢ Camadas LSTM: {NUM_LAYERS}\n",
    "   ‚Ä¢ Unidades ocultas: {HIDDEN_DIM} (bidirecional = {HIDDEN_DIM*2} total)\n",
    "   ‚Ä¢ Embedding: {EMBEDDING_DIM} dimens√µes\n",
    "   ‚Ä¢ Dropout: {DROPOUT}\n",
    "   ‚Ä¢ Classes: {len(label_encoder.classes_)}\n",
    "\n",
    "üìà DADOS:\n",
    "   ‚Ä¢ Tamanho do dataset: {len(df):,} amostras\n",
    "   ‚Ä¢ Vocabul√°rio: {len(vocabulary):,} palavras\n",
    "   ‚Ä¢ Comprimento m√°ximo: {MAX_SEQUENCE_LENGTH} tokens\n",
    "   ‚Ä¢ Divis√£o: {len(X_train)} treino, {len(X_val)} valida√ß√£o, {len(X_test)} teste\n",
    "\n",
    "üîß CONFIGURA√á√ÉO DE TREINAMENTO:\n",
    "   ‚Ä¢ Otimizador: AdamW (LR={LEARNING_RATE}, Weight Decay={WEIGHT_DECAY})\n",
    "   ‚Ä¢ Batch Size: {BATCH_SIZE}\n",
    "   ‚Ä¢ √âpocas m√°ximas: {MAX_EPOCHS}\n",
    "   ‚Ä¢ Early Stopping: Paci√™ncia de {PATIENCE} √©pocas\n",
    "   ‚Ä¢ Gradient Clipping: {GRADIENT_CLIP}\n",
    "\n",
    "üíæ ARQUIVOS SALVOS:\n",
    "   ‚Ä¢ Modelo: {model_path}\n",
    "   ‚Ä¢ Vocabul√°rio: {vocab_path}\n",
    "   ‚Ä¢ Label Encoder: {label_encoder_path}\n",
    "   ‚Ä¢ M√©tricas: {metrics_path}\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úÖ IMPLEMENTA√á√ÉO CONCLU√çDA COM SUCESSO!\")\n",
    "print(\"O modelo LSTM bidirecional com aten√ß√£o est√° pronto para classifica√ß√£o de peti√ß√µes.\")\n",
    "print(\"Use a fun√ß√£o predict_petition() para fazer predi√ß√µes em novos textos.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
