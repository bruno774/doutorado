{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47fe916b",
   "metadata": {},
   "source": [
    "# Prototipagem CNN para Classifica√ß√£o de Desfecho de Peti√ß√µes Jur√≠dicas\n",
    "\n",
    "Este notebook implementa uma **Rede Neural Convolucional Profunda (Deep CNN)** para classifica√ß√£o do resultado/desfecho prov√°vel de peti√ß√µes jur√≠dicas.\n",
    "\n",
    "## Objetivo\n",
    "Prever o desfecho de uma peti√ß√£o (Deferida, Indeferida, Parcialmente Deferida) baseado no conte√∫do textual e metadados da peti√ß√£o.\n",
    "\n",
    "## Arquitetura\n",
    "- **Entrada**: Texto processado (embeddings word2vec/GloVe)\n",
    "- **Camadas Convolucionais 1D**: Extra√ß√£o de padr√µes textuais locais\n",
    "- **Pooling**: Redu√ß√£o de dimensionalidade\n",
    "- **Regulariza√ß√£o**: Dropout + BatchNormalization\n",
    "- **Camadas Densas**: Classifica√ß√£o final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d552852b",
   "metadata": {},
   "source": [
    "## 1. Importar Bibliotecas Necess√°rias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865241e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling1D, Input, concatenate\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Processamento de texto\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "\n",
    "# M√©tricas e valida√ß√£o\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    roc_curve, auc\n",
    ")\n",
    "\n",
    "# Plotagem e visualiza√ß√£o\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Download de recursos NLTK\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "print(\"‚úì Todas as bibliotecas importadas com sucesso!\")\n",
    "print(f\"‚úì TensorFlow vers√£o: {tf.__version__}\")\n",
    "print(f\"‚úì GPU dispon√≠vel: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7164f68",
   "metadata": {},
   "source": [
    "## 2. Carregar e Explorar os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b802ee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho do banco de dados SQLite\n",
    "DB_PATH = \"/gdrive/MyDrive/Colab Notebooks/JFRN/split_dados_01.sqlite3\"  # Ajuste o caminho conforme necess√°rio\n",
    "TABLE_NAME = \"peticoes\"  # Ajuste o nome da tabela\n",
    "\n",
    "# Conectar ao banco e carregar dados\n",
    "try:\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    query = f\"SELECT * FROM {TABLE_NAME} LIMIT 1000\"  # Limite inicial para prototipagem\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    print(f\"‚úì Dados carregados com sucesso!\")\n",
    "    print(f\"  - Dimens√µes: {df.shape[0]} linhas x {df.shape[1]} colunas\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Erro ao carregar dados: {e}\")\n",
    "    print(\"\\nCriando dataset de exemplo para demonstra√ß√£o...\")\n",
    "    \n",
    "    # Dataset de exemplo para demonstra√ß√£o\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    example_texts = [\n",
    "        \"A peti√ß√£o solicita a revis√£o da senten√ßa anterior com base em novos documentos.\",\n",
    "        \"Recurso de apela√ß√£o contra a decis√£o de primeira inst√¢ncia.\",\n",
    "        \"Mo√ß√£o para reconsidera√ß√£o do pedido inicial.\",\n",
    "        \"A√ß√£o ordin√°ria para cobran√ßa de valores contratados.\",\n",
    "        \"Demanda por responsabilidade civil.\",\n",
    "        \"Peti√ß√£o para anula√ß√£o de ato administrativo.\",\n",
    "        \"Recurso extraordin√°rio questionando constitucionalidade.\",\n",
    "        \"Processo trabalhista por rescis√£o contratual.\",\n",
    "    ]\n",
    "    \n",
    "    texts = [np.random.choice(example_texts) for _ in range(n_samples)]\n",
    "    outcomes = np.random.choice(['Deferida', 'Indeferida', 'Parcialmente_Deferida'], n_samples)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'texto_peticao': texts,\n",
    "        'desfecho': outcomes,\n",
    "        'data': pd.date_range('2020-01-01', periods=n_samples, freq='D'),\n",
    "        'valor': np.random.uniform(1000, 100000, n_samples)\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úì Dataset de exemplo criado: {df.shape[0]} linhas x {df.shape[1]} colunas\")\n",
    "\n",
    "# Exibir informa√ß√µes do dataset\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INFORMA√á√ïES DO DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(df.head(10))\n",
    "print(f\"\\nTipos de dados:\\n{df.dtypes}\")\n",
    "print(f\"\\nValores nulos:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a2cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise explorat√≥ria da vari√°vel alvo (desfecho)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AN√ÅLISE DA VARI√ÅVEL ALVO: DESFECHO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Coluna de desfecho (ajuste conforme necess√°rio)\n",
    "target_column = 'desfecho'  # ou 'resultado', 'outcome', etc.\n",
    "\n",
    "# Verificar se a coluna existe\n",
    "if target_column not in df.columns:\n",
    "    print(f\"‚ö† Coluna '{target_column}' n√£o encontrada.\")\n",
    "    print(f\"Colunas dispon√≠veis: {df.columns.tolist()}\")\n",
    "    # Tentar encontrar coluna similar\n",
    "    possible_cols = [col for col in df.columns if 'result' in col.lower() or 'desfecho' in col.lower() or 'outcome' in col.lower()]\n",
    "    if possible_cols:\n",
    "        target_column = possible_cols[0]\n",
    "        print(f\"Usando coluna: {target_column}\")\n",
    "else:\n",
    "    print(f\"‚úì Usando coluna: {target_column}\")\n",
    "\n",
    "# Distribui√ß√£o das classes\n",
    "class_distribution = df[target_column].value_counts()\n",
    "print(f\"\\nDistribui√ß√£o de classes:\")\n",
    "print(class_distribution)\n",
    "print(f\"\\nPercentual:\")\n",
    "print((df[target_column].value_counts(normalize=True) * 100).round(2))\n",
    "\n",
    "# Visualizar distribui√ß√£o\n",
    "plt.figure(figsize=(10, 5))\n",
    "class_distribution.plot(kind='bar', edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribui√ß√£o de Desfechos nas Peti√ß√µes', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Desfecho')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verificar desbalanceamento\n",
    "print(f\"\\nRaz√£o maior/menor classe: {class_distribution.max() / class_distribution.min():.2f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ba62b0",
   "metadata": {},
   "source": [
    "## 3. Pr√©-processamento e Normaliza√ß√£o de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2882f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para pr√©-processamento de texto\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Pr√©-processar texto para treinamento de CNN\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Converter para string e lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remover URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remover emails\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remover n√∫meros\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remover pontua√ß√£o especial\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Tokeniza√ß√£o\n",
    "    tokens = word_tokenize(text, language='portuguese')\n",
    "    \n",
    "    # Remover stopwords portugu√™s\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = SnowballStemmer('portuguese')\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Aplicar pr√©-processamento\n",
    "print(\"Pr√©-processando textos...\")\n",
    "text_column = 'texto_peticao'  # Ajuste conforme necess√°rio\n",
    "\n",
    "if text_column not in df.columns:\n",
    "    possible_cols = [col for col in df.columns if 'text' in col.lower() or 'peticao' in col.lower()]\n",
    "    if possible_cols:\n",
    "        text_column = possible_cols[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Coluna de texto n√£o encontrada. Colunas dispon√≠veis: {df.columns.tolist()}\")\n",
    "\n",
    "df['texto_processado'] = df[text_column].apply(preprocess_text)\n",
    "\n",
    "print(\"‚úì Textos pr√©-processados!\")\n",
    "print(f\"\\nExemplo antes: {df[text_column].iloc[0][:100]}...\")\n",
    "print(f\"Exemplo depois: {df['texto_processado'].iloc[0][:100]}...\")\n",
    "\n",
    "# Estat√≠sticas de comprimento\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ESTAT√çSTICAS DE COMPRIMENTO DOS TEXTOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "text_lengths = df['texto_processado'].str.split().str.len()\n",
    "print(f\"Comprimento m√©dio: {text_lengths.mean():.2f} palavras\")\n",
    "print(f\"Comprimento m√≠nimo: {text_lengths.min()}\")\n",
    "print(f\"Comprimento m√°ximo: {text_lengths.max()}\")\n",
    "print(f\"Mediana: {text_lengths.median():.2f}\")\n",
    "print(f\"Desvio padr√£o: {text_lengths.std():.2f}\")\n",
    "\n",
    "# Visualizar distribui√ß√£o de comprimentos\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.hist(text_lengths, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('N√∫mero de Palavras')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "plt.title('Distribui√ß√£o do Comprimento dos Textos Pr√©-processados')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc9dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeniza√ß√£o e sequ√™ncia de palavras\n",
    "VOCAB_SIZE = 5000  # Tamanho do vocabul√°rio\n",
    "MAX_SEQUENCE_LENGTH = 200  # Comprimento m√°ximo das sequ√™ncias\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOKENIZA√á√ÉO E SEQU√äNCIAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Criar tokenizer\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df['texto_processado'])\n",
    "\n",
    "# Converter textos para sequ√™ncias\n",
    "sequences = tokenizer.texts_to_sequences(df['texto_processado'])\n",
    "\n",
    "# Fazer padding\n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "print(f\"‚úì Tokeniza√ß√£o conclu√≠da!\")\n",
    "print(f\"  - Tamanho do vocabul√°rio: {len(tokenizer.word_index)}\")\n",
    "print(f\"  - Sequ√™ncias criadas: {X.shape}\")\n",
    "print(f\"  - Comprimento m√°ximo: {MAX_SEQUENCE_LENGTH}\")\n",
    "\n",
    "# Codificar labels (desfecho)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df[target_column])\n",
    "y_labels = label_encoder.classes_\n",
    "\n",
    "print(f\"\\n‚úì Vari√°vel alvo codificada!\")\n",
    "print(f\"  - Classes: {y_labels}\")\n",
    "print(f\"  - Codifica√ß√£o: {dict(zip(y_labels, label_encoder.transform(y_labels)))}\")\n",
    "\n",
    "# Dividir dados em train/validation/test\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIVIS√ÉO TRAIN/VALIDATION/TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.15, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Conjunto de treino: {X_train.shape[0]} amostras ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Conjunto de valida√ß√£o: {X_val.shape[0]} amostras ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Conjunto de teste: {X_test.shape[0]} amostras ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Converter para one-hot encoding\n",
    "num_classes = len(y_labels)\n",
    "y_train_cat = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val_cat = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test_cat = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"\\nShape dos labels (one-hot): {y_train_cat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d34ab",
   "metadata": {},
   "source": [
    "## 4. Definir Arquitetura da Rede Convolucional Profunda\n",
    "\n",
    "### Arquitetura CNN 1D para Classifica√ß√£o de Texto\n",
    "- **Embedding Layer**: Converte √≠ndices de palavras em vetores densos\n",
    "- **M√∫ltiplas Camadas Convolucionais 1D**: Com diferentes tamanhos de filtro (3, 4, 5)\n",
    "- **Batch Normalization**: Normaliza√ß√£o entre camadas para estabilidade\n",
    "- **Global Max Pooling**: Redu√ß√£o de dimensionalidade\n",
    "- **Dropout**: Regulariza√ß√£o para evitar overfitting\n",
    "- **Camadas Densas**: Classifica√ß√£o final com ativa√ß√£o softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deep_cnn_model(vocab_size, max_length, num_classes, embedding_dim=128):\n",
    "    \"\"\"\n",
    "    Construir modelo CNN profundo para classifica√ß√£o de texto\n",
    "    \n",
    "    Par√¢metros:\n",
    "    - vocab_size: Tamanho do vocabul√°rio\n",
    "    - max_length: Comprimento m√°ximo das sequ√™ncias\n",
    "    - num_classes: N√∫mero de classes\n",
    "    - embedding_dim: Dimensionalidade do embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    # Camada de entrada\n",
    "    input_layer = Input(shape=(max_length,))\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding = Embedding(vocab_size, embedding_dim, input_length=max_length)(input_layer)\n",
    "    \n",
    "    # M√∫ltiplas camadas convolucionais com diferentes tamanhos de filtro\n",
    "    filter_sizes = [3, 4, 5, 7]\n",
    "    num_filters = 100\n",
    "    conv_layers = []\n",
    "    \n",
    "    for filter_size in filter_sizes:\n",
    "        conv = Conv1D(\n",
    "            filters=num_filters,\n",
    "            kernel_size=filter_size,\n",
    "            activation='relu',\n",
    "            kernel_regularizer=keras.regularizers.l2(0.001)\n",
    "        )(embedding)\n",
    "        \n",
    "        # Batch Normalization\n",
    "        conv = BatchNormalization()(conv)\n",
    "        \n",
    "        # Max Pooling\n",
    "        conv = MaxPooling1D(pool_size=2)(conv)\n",
    "        \n",
    "        # Dropout\n",
    "        conv = Dropout(0.3)(conv)\n",
    "        \n",
    "        # Global Max Pooling\n",
    "        conv = GlobalAveragePooling1D()(conv)\n",
    "        \n",
    "        conv_layers.append(conv)\n",
    "    \n",
    "    # Concatenar outputs das camadas convolucionais\n",
    "    merged = concatenate(conv_layers) if len(conv_layers) > 1 else conv_layers[0]\n",
    "    \n",
    "    # Camadas densas\n",
    "    dense = Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(merged)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = Dropout(0.4)(dense)\n",
    "    \n",
    "    dense = Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(dense)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = Dropout(0.3)(dense)\n",
    "    \n",
    "    dense = Dense(64, activation='relu')(dense)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "    \n",
    "    # Camada de sa√≠da\n",
    "    output = Dense(num_classes, activation='softmax')(dense)\n",
    "    \n",
    "    # Criar modelo\n",
    "    model = models.Model(inputs=input_layer, outputs=output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Construir modelo\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONSTRUINDO ARQUITETURA CNN PROFUNDA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "\n",
    "model = build_deep_cnn_model(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_classes=num_classes,\n",
    "    embedding_dim=EMBEDDING_DIM\n",
    ")\n",
    "\n",
    "# Exibir resumo da arquitetura\n",
    "print(\"\\n‚úì Modelo constru√≠do com sucesso!\")\n",
    "print(\"\\nArquitetura do Modelo:\")\n",
    "print(\"-\" * 80)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5008b3d2",
   "metadata": {},
   "source": [
    "## 5. Compilar o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc4fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPILANDO MODELO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Configurar otimizador e fun√ß√£o de perda\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "loss_fn = 'categorical_crossentropy'\n",
    "metrics_list = ['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    "\n",
    "# Compilar modelo\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_fn,\n",
    "    metrics=metrics_list\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Modelo compilado com sucesso!\")\n",
    "print(f\"  - Otimizador: Adam (lr=0.001)\")\n",
    "print(f\"  - Fun√ß√£o de perda: {loss_fn}\")\n",
    "print(f\"  - M√©tricas: {[m if isinstance(m, str) else m.__class__.__name__ for m in metrics_list]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3db9e32",
   "metadata": {},
   "source": [
    "## 6. Treinar o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TREINANDO MODELO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Configura√ß√µes de treinamento\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "\n",
    "# Treinar modelo\n",
    "print(f\"\\nPar√¢metros:\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - √âpocas: {EPOCHS}\")\n",
    "print(f\"  - Amostras de treino: {X_train.shape[0]}\")\n",
    "print(f\"  - Amostras de valida√ß√£o: {X_val.shape[0]}\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Treinamento conclu√≠do!\")\n",
    "print(f\"  - √âpocas executadas: {len(history.history['loss'])}\")\n",
    "print(f\"  - Melhor val_loss: {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"  - Melhor val_accuracy: {max(history.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8d51f1",
   "metadata": {},
   "source": [
    "## 7. Avaliar o Desempenho do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c28f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AVALIA√á√ÉO DO MODELO NO CONJUNTO DE TESTE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Avaliar no conjunto de teste\n",
    "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(\n",
    "    X_test, y_test_cat, verbose=0\n",
    ")\n",
    "\n",
    "print(f\"\\nM√©tricas no Conjunto de Teste:\")\n",
    "print(f\"  - Loss: {test_loss:.4f}\")\n",
    "print(f\"  - Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"  - Precision: {test_precision:.4f}\")\n",
    "print(f\"  - Recall: {test_recall:.4f}\")\n",
    "print(f\"  - F1-Score: {2 * (test_precision * test_recall) / (test_precision + test_recall):.4f}\")\n",
    "\n",
    "# Fazer predi√ß√µes\n",
    "y_pred_proba = model.predict(X_test, verbose=0)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Relat√≥rio de classifica√ß√£o\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RELAT√ìRIO DE CLASSIFICA√á√ÉO DETALHADO\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_test, y_pred, target_names=y_labels, digits=4))\n",
    "\n",
    "# Matriz de confus√£o\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MATRIZ DE CONFUS√ÉO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\n\", cm)\n",
    "\n",
    "# Calcular F1-Score por classe\n",
    "f1_scores = {}\n",
    "for i, label in enumerate(y_labels):\n",
    "    f1 = f1_score(y_test == i, y_pred == i, average='binary')\n",
    "    f1_scores[label] = f1\n",
    "\n",
    "print(\"\\nF1-Score por classe:\")\n",
    "for label, f1 in f1_scores.items():\n",
    "    print(f\"  - {label}: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265134bb",
   "metadata": {},
   "source": [
    "## 8. Realizar Predi√ß√µes em Novos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9783341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_petition_outcome(text, model, tokenizer, label_encoder, max_length=MAX_SEQUENCE_LENGTH):\n",
    "    \"\"\"\n",
    "    Fazer predi√ß√£o de desfecho para uma nova peti√ß√£o\n",
    "    \"\"\"\n",
    "    # Pr√©-processar texto\n",
    "    processed_text = preprocess_text(text)\n",
    "    \n",
    "    # Converter para sequ√™ncia\n",
    "    sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "    padded = pad_sequences(sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "    \n",
    "    # Fazer predi√ß√£o\n",
    "    prediction_proba = model.predict(padded, verbose=0)[0]\n",
    "    predicted_class = np.argmax(prediction_proba)\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_class])[0]\n",
    "    \n",
    "    return {\n",
    "        'texto': text[:100] + '...' if len(text) > 100 else text,\n",
    "        'desfecho_predito': predicted_label,\n",
    "        'confianca': prediction_proba[predicted_class],\n",
    "        'probabilidades': {label: float(prob) for label, prob in zip(label_encoder.classes_, prediction_proba)}\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREDI√á√ïES EM NOVOS DADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Testar com exemplos do conjunto de teste\n",
    "print(\"\\nExemplos de predi√ß√µes do conjunto de teste:\\n\")\n",
    "\n",
    "for i in range(min(5, len(X_test))):\n",
    "    texto_original = df[text_column].iloc[-len(X_test) + i]\n",
    "    resultado_real = y_labels[y_test[i]]\n",
    "    resultado_predito = y_labels[y_pred[i]]\n",
    "    confianca = y_pred_proba[i][y_pred[i]]\n",
    "    \n",
    "    print(f\"Exemplo {i+1}:\")\n",
    "    print(f\"  Texto: {texto_original[:80]}...\")\n",
    "    print(f\"  Resultado real: {resultado_real}\")\n",
    "    print(f\"  Resultado predito: {resultado_predito}\")\n",
    "    print(f\"  Confian√ßa: {confianca:.4f}\")\n",
    "    print(f\"  Acerto: {'‚úì' if resultado_real == resultado_predito else '‚úó'}\")\n",
    "    print()\n",
    "\n",
    "# Testar com novos textos de exemplo\n",
    "print(\"\\nPredi√ß√µes em novos textos de exemplo:\\n\")\n",
    "\n",
    "novos_textos = [\n",
    "    \"Recurso extraordin√°rio fundamentado em viola√ß√£o de direito constitucional e jurisprud√™ncia consolidada\",\n",
    "    \"Peti√ß√£o inicial requerendo condena√ß√£o por responsabilidade civil e danos morais\",\n",
    "    \"Mo√ß√£o para reconsidera√ß√£o apresentando novas provas documentais\"\n",
    "]\n",
    "\n",
    "for texto in novos_textos:\n",
    "    resultado = predict_petition_outcome(texto, model, tokenizer, label_encoder)\n",
    "    print(f\"Texto: {resultado['texto']}\")\n",
    "    print(f\"Desfecho predito: {resultado['desfecho_predito']}\")\n",
    "    print(f\"Confian√ßa: {resultado['confianca']:.4f}\")\n",
    "    print(f\"Probabilidades por classe:\")\n",
    "    for classe, prob in resultado['probabilidades'].items():\n",
    "        print(f\"  - {classe}: {prob:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed0697e",
   "metadata": {},
   "source": [
    "## 9. Visualizar Resultados e M√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd79b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZA√á√ïES DE RESULTADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Hist√≥rico de treinamento (Loss)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('√âpoca', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Hist√≥rico de Loss durante Treinamento', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('√âpoca', fontsize=12)\n",
    "axes[1].set_ylabel('Acur√°cia', fontsize=12)\n",
    "axes[1].set_title('Hist√≥rico de Acur√°cia durante Treinamento', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Matriz de Confus√£o\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=y_labels)\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "plt.title('Matriz de Confus√£o - Conjunto de Teste', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. M√©tricas por classe\n",
    "metrics_by_class = {}\n",
    "for i, label in enumerate(y_labels):\n",
    "    tn = cm.sum() - (cm[i, :].sum() + cm[:, i].sum() - cm[i, i])\n",
    "    fp = cm[:, i].sum() - cm[i, i]\n",
    "    fn = cm[i, :].sum() - cm[i, i]\n",
    "    tp = cm[i, i]\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    metrics_by_class[label] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'support': (y_test == i).sum()\n",
    "    }\n",
    "\n",
    "# Plotar m√©tricas por classe\n",
    "metrics_df = pd.DataFrame(metrics_by_class).T\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(y_labels))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, metrics_df['precision'], width, label='Precis√£o', alpha=0.8)\n",
    "ax.bar(x, metrics_df['recall'], width, label='Recall', alpha=0.8)\n",
    "ax.bar(x + width, metrics_df['f1'], width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Classe', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('M√©tricas de Desempenho por Classe', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(y_labels)\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_ylim([0, 1.1])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Visualiza√ß√µes geradas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2122d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZA√á√ïES ADICIONAIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 4. Distribui√ß√£o de probabilidades preditas\n",
    "fig, axes = plt.subplots(1, len(y_labels), figsize=(15, 5))\n",
    "\n",
    "for idx, label in enumerate(y_labels):\n",
    "    class_probs = y_pred_proba[y_test == idx, idx]\n",
    "    axes[idx].hist(class_probs, bins=30, edgecolor='black', alpha=0.7, color=f'C{idx}')\n",
    "    axes[idx].set_xlabel('Probabilidade', fontsize=11)\n",
    "    axes[idx].set_ylabel('Frequ√™ncia', fontsize=11)\n",
    "    axes[idx].set_title(f'Confian√ßa - {label}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlim([0, 1])\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Curva ROC (One-vs-Rest) para classifica√ß√£o multiclasse\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_test_bin = label_binarize(y_test, classes=range(num_classes))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, num_classes))\n",
    "for i, label in enumerate(y_labels):\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, color=colors[i], lw=2, label=f'{label} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Classificador Aleat√≥rio')\n",
    "ax.set_xlabel('Taxa de Falso Positivo', fontsize=12)\n",
    "ax.set_ylabel('Taxa de Verdadeiro Positivo', fontsize=12)\n",
    "ax.set_title('Curvas ROC - One-vs-Rest', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Exemplos de predi√ß√µes corretas e incorretas\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 8))\n",
    "fig.suptitle('Exemplos de Predi√ß√µes: Corretas (superior) e Incorretas (inferior)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Corretas\n",
    "correct_indices = np.where(y_pred == y_test)[0]\n",
    "for i in range(min(3, len(correct_indices))):\n",
    "    idx = correct_indices[i]\n",
    "    ax = axes[0, i]\n",
    "    \n",
    "    # Criar visualiza√ß√£o de confian√ßa\n",
    "    classes = y_labels\n",
    "    probs = y_pred_proba[idx]\n",
    "    ax.barh(classes, probs, color=['green' if p == max(probs) else 'lightgray' for p in probs])\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_xlabel('Probabilidade', fontsize=10)\n",
    "    ax.set_title(f'‚úì Correto: {y_labels[y_test[idx]]}', fontsize=11, fontweight='bold', color='green')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Incorretas\n",
    "incorrect_indices = np.where(y_pred != y_test)[0]\n",
    "for i in range(min(3, len(incorrect_indices))):\n",
    "    idx = incorrect_indices[i]\n",
    "    ax = axes[1, i]\n",
    "    \n",
    "    # Criar visualiza√ß√£o de confian√ßa\n",
    "    classes = y_labels\n",
    "    probs = y_pred_proba[idx]\n",
    "    colors = ['red' if j == y_pred[idx] else ('orange' if j == y_test[idx] else 'lightgray') \n",
    "              for j in range(len(classes))]\n",
    "    ax.barh(classes, probs, color=colors)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_xlabel('Probabilidade', fontsize=10)\n",
    "    title_text = f'‚úó Incorreto\\nReal: {y_labels[y_test[idx]]}, Predito: {y_labels[y_pred[idx]]}'\n",
    "    ax.set_title(title_text, fontsize=11, fontweight='bold', color='red')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Remover subplots vazios se necess√°rio\n",
    "for i in range(min(3, len(correct_indices)), 3):\n",
    "    fig.delaxes(axes[0, i])\n",
    "for i in range(min(3, len(incorrect_indices)), 3):\n",
    "    fig.delaxes(axes[1, i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Todas as visualiza√ß√µes geradas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf501a91",
   "metadata": {},
   "source": [
    "## 10. Resumo e Recomenda√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a3e6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMO DO MODELO E RECOMENDA√á√ïES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä RESUMO GERAL:\")\n",
    "print(f\"\"\"\n",
    "  ‚Ä¢ Modelo: CNN 1D com 4 camadas convolucionais paralelas\n",
    "  ‚Ä¢ Tamanho vocabul√°rio: {VOCAB_SIZE}\n",
    "  ‚Ä¢ Dimens√£o embedding: {EMBEDDING_DIM}\n",
    "  ‚Ä¢ Comprimento m√°ximo sequ√™ncia: {MAX_SEQUENCE_LENGTH}\n",
    "  ‚Ä¢ Total de par√¢metros: {model.count_params():,}\n",
    "  ‚Ä¢ √âpocas treinadas: {len(history.history['loss'])}\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìà DESEMPENHO NO CONJUNTO DE TESTE:\")\n",
    "print(f\"\"\"\n",
    "  ‚Ä¢ Acur√°cia: {test_accuracy:.4f}\n",
    "  ‚Ä¢ Precis√£o: {test_precision:.4f}\n",
    "  ‚Ä¢ Recall: {test_recall:.4f}\n",
    "  ‚Ä¢ F1-Score: {2 * (test_precision * test_recall) / (test_precision + test_recall):.4f}\n",
    "\"\"\")\n",
    "\n",
    "print(\"üéØ RECOMENDA√á√ïES PARA MELHORAR O MODELO:\")\n",
    "recommendations = [\n",
    "    \"1. Aumentar tamanho do dataset - mais dados melhoram generaliza√ß√£o\",\n",
    "    \"2. Tentar arquitetura Transformer/BERT - melhor compreens√£o contextual\",\n",
    "    \"3. Implementar class weights - lidar com desbalanceamento de classes\",\n",
    "    \"4. Fazer feature engineering - incluir metadados (data, √°rea jur√≠dica, etc)\",\n",
    "    \"5. Usar word embeddings pr√©-treinados (Word2Vec, GloVe, FastText)\",\n",
    "    \"6. Testar diferentes tamanhos de embedding e filtros\",\n",
    "    \"7. Aumentar dropouts se houver sinais de overfitting\",\n",
    "    \"8. Fazer abla√ß√£o de arquitetura - remover/adicionar camadas\",\n",
    "    \"9. Implementar ensemble com modelos complementares\",\n",
    "    \"10. Valida√ß√£o cruzada estratificada para estimativa mais robusta\"\n",
    "]\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(f\"  {rec}\")\n",
    "\n",
    "print(\"\\nüíæ SALVANDO MODELO:\")\n",
    "# Salvar modelo\n",
    "model_path = '/Users/bruno.silva/Projects/doutorado/cnn-classificador-peticoes/modelo_cnn_desfecho.h5'\n",
    "model.save(model_path)\n",
    "print(f\"  ‚úì Modelo salvo em: {model_path}\")\n",
    "\n",
    "# Salvar tokenizer\n",
    "import pickle\n",
    "tokenizer_path = '/Users/bruno.silva/Projects/doutorado/cnn-classificador-peticoes/tokenizer_desfecho.pkl'\n",
    "with open(tokenizer_path, 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(f\"  ‚úì Tokenizer salvo em: {tokenizer_path}\")\n",
    "\n",
    "# Salvar label encoder\n",
    "encoder_path = '/Users/bruno.silva/Projects/doutorado/cnn-classificador-peticoes/label_encoder_desfecho.pkl'\n",
    "with open(encoder_path, 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "print(f\"  ‚úì Label Encoder salvo em: {encoder_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ PROTOTIPAGEM CONCLU√çDA COM SUCESSO!\")\n",
    "print(\"\\nPr√≥ximos passos:\")\n",
    "print(\"  1. Refinar modelo com dados reais\")\n",
    "print(\"  2. Explorar arquiteturas mais avan√ßadas (Transformers)\")\n",
    "print(\"  3. Implementar API de predi√ß√£o em produ√ß√£o\")\n",
    "print(\"  4. Monitorar performance em ambiente de produ√ß√£o\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
